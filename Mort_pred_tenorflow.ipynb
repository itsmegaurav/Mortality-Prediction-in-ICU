{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/gaurav/Documents/Mor_pred/ayush/train.csv')\n",
    "labels=pd.read_csv('/home/gaurav/Documents/Mor_pred/ayush/labels_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Age</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>DiasABP</th>\n",
       "      <th>...</th>\n",
       "      <th>RespRate</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>SysABP</th>\n",
       "      <th>Temp</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>Urine</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Weight</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>54</td>\n",
       "      <td>2.973333</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>58.795833</td>\n",
       "      <td>...</td>\n",
       "      <td>17.428571</td>\n",
       "      <td>97.250000</td>\n",
       "      <td>116.891892</td>\n",
       "      <td>37.357143</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>171.052632</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>80.060976</td>\n",
       "      <td>7.387273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>76</td>\n",
       "      <td>2.973333</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>58.897059</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>96.833333</td>\n",
       "      <td>113.411765</td>\n",
       "      <td>36.939130</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>151.560976</td>\n",
       "      <td>11.266667</td>\n",
       "      <td>80.670588</td>\n",
       "      <td>7.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>199.5</td>\n",
       "      <td>44</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2.9</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>125.687500</td>\n",
       "      <td>37.800000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>124.951220</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>7.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>68</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>58.795833</td>\n",
       "      <td>...</td>\n",
       "      <td>15.457627</td>\n",
       "      <td>97.250000</td>\n",
       "      <td>116.891892</td>\n",
       "      <td>36.223077</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>545.833333</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>7.387273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58.795833</td>\n",
       "      <td>...</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>97.250000</td>\n",
       "      <td>116.891892</td>\n",
       "      <td>36.880000</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>62.131579</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>80.060976</td>\n",
       "      <td>7.387273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ALP   ALT    AST  Age   Albumin        BUN  Bilirubin  Cholesterol  \\\n",
       "0   77.0  31.0   46.0   54  2.973333  10.500000        0.7        154.0   \n",
       "1   77.0  31.0   46.0   76  2.973333  18.333333        0.7        154.0   \n",
       "2  116.0  83.0  199.5   44  2.500000   4.666667        2.9        154.0   \n",
       "3  105.0  12.0   15.0   68  4.400000  17.666667        0.2        154.0   \n",
       "4   77.0  31.0   46.0   88  3.300000  35.000000        0.7        154.0   \n",
       "\n",
       "   Creatinine    DiasABP    ...      RespRate       SaO2      SysABP  \\\n",
       "0    0.750000  58.795833    ...     17.428571  97.250000  116.891892   \n",
       "1    1.100000  58.897059    ...     19.000000  96.833333  113.411765   \n",
       "2    0.333333  67.125000    ...     19.000000  95.000000  125.687500   \n",
       "3    0.766667  58.795833    ...     15.457627  97.250000  116.891892   \n",
       "4    1.000000  58.795833    ...     19.166667  97.250000  116.891892   \n",
       "\n",
       "        Temp  TroponinI  TroponinT       Urine        WBC     Weight        pH  \n",
       "0  37.357143        2.1       0.14  171.052632  10.300000  80.060976  7.387273  \n",
       "1  36.939130        2.1       0.14  151.560976  11.266667  80.670588  7.395000  \n",
       "2  37.800000        2.1       0.14  124.951220   4.700000  56.700000  7.495000  \n",
       "3  36.223077        2.1       0.14  545.833333   9.400000  84.600000  7.387273  \n",
       "4  36.880000        2.1       0.14   62.131579   4.300000  80.060976  7.387273  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df_res,labels_res=SMOTE(random_state=9).fit_sample(df,labels) #balancing the data\n",
    "df_res=pd.DataFrame(df_res)\n",
    "labels_res=pd.DataFrame(labels_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder(sparse=False)\n",
    "onehot_encoded = enc.fit_transform(labels_res)\n",
    "\n",
    "labels_res=onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "'''corrmat=df.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(corrmat,vmax=.6,square=True)\n",
    "plt.show()'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_labels,test_labels=train_test_split(df_res,labels_res,random_state=3,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_xs, batch_ys = tf.train.batch([train_data, train_labels],batch_size=100)\n",
    "\n",
    "n_hidden_1=60\n",
    "n_hidden_2=60\n",
    "n_hidden_3=60\n",
    "n_hidden_4=60\n",
    "\n",
    "x=tf.placeholder(tf.float32,shape=[None,42])\n",
    "y_= tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "\n",
    "w=tf.Variable(tf.zeros([42,2]))\n",
    "b=tf.Variable(tf.zeros([2]))\n",
    "\n",
    "def multilayer_perceptron(x,weights,biases):\n",
    "    \n",
    "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1=tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2=tf.nn.sigmoid(layer_2)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,weights['h3']),biases['b3'])\n",
    "    layer_3=tf.nn.sigmoid(layer_3)\n",
    "    \n",
    "    layer_4=tf.add(tf.matmul(layer_3,weights['h4']),biases['b4'])\n",
    "    layer_4=tf.nn.relu(layer_4)\n",
    "    \n",
    "    \n",
    "    out_layer=tf.add(tf.matmul(layer_4,weights['out']),biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "weights={\n",
    "            'h1':tf.Variable(tf.truncated_normal([42,n_hidden_1])),\n",
    "            'h2':tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
    "            'h3':tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3])),\n",
    "            'h4':tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4])),\n",
    "            'out':tf.Variable(tf.truncated_normal([n_hidden_4,2]))\n",
    "}\n",
    "\n",
    "biases={\n",
    "            'b1':tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "            'b2':tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "            'b3':tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "            'b4':tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "            'out':tf.Variable(tf.truncated_normal([2]))\n",
    "}\n",
    "\n",
    "y=multilayer_perceptron(x,weights,biases)\n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y))\n",
    "train_step=tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_history=[]\n",
    "cost_history=[]\n",
    "accuracy_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - cost: 10.1045 -MSE: 209.595620367 -train accuracy: 0.50127\n",
      "epoch: 1 - cost: 1.56659 -MSE: 117.753665234 -train accuracy: 0.521045\n",
      "epoch: 2 - cost: 1.50624 -MSE: 116.837039258 -train accuracy: 0.532293\n",
      "epoch: 3 - cost: 1.51259 -MSE: 117.838177217 -train accuracy: 0.51016\n",
      "epoch: 4 - cost: 2.02456 -MSE: 116.024502442 -train accuracy: 0.511792\n",
      "epoch: 5 - cost: 4.6514 -MSE: 144.27980891 -train accuracy: 0.499093\n",
      "epoch: 6 - cost: 4.38745 -MSE: 124.949171315 -train accuracy: 0.500907\n",
      "epoch: 7 - cost: 4.65309 -MSE: 139.691511314 -train accuracy: 0.499274\n",
      "epoch: 8 - cost: 4.14449 -MSE: 117.429488311 -train accuracy: 0.500907\n",
      "epoch: 9 - cost: 4.6097 -MSE: 134.14253989 -train accuracy: 0.499274\n",
      "epoch: 10 - cost: 3.93651 -MSE: 108.517912548 -train accuracy: 0.500907\n",
      "epoch: 11 - cost: 4.50546 -MSE: 127.533649534 -train accuracy: 0.499274\n",
      "epoch: 12 - cost: 3.80035 -MSE: 99.0731934463 -train accuracy: 0.501089\n",
      "epoch: 13 - cost: 4.35027 -MSE: 122.466773522 -train accuracy: 0.499274\n",
      "epoch: 14 - cost: 3.77366 -MSE: 95.0707066913 -train accuracy: 0.501089\n",
      "epoch: 15 - cost: 4.19672 -MSE: 118.495545256 -train accuracy: 0.499274\n",
      "epoch: 16 - cost: 3.7589 -MSE: 92.8091497688 -train accuracy: 0.501089\n",
      "epoch: 17 - cost: 4.05345 -MSE: 114.985834997 -train accuracy: 0.499274\n",
      "epoch: 18 - cost: 3.74466 -MSE: 90.9321592639 -train accuracy: 0.501089\n",
      "epoch: 19 - cost: 3.92244 -MSE: 111.788616271 -train accuracy: 0.499274\n",
      "epoch: 20 - cost: 3.72951 -MSE: 89.276425544 -train accuracy: 0.501089\n",
      "epoch: 21 - cost: 3.80371 -MSE: 108.979550493 -train accuracy: 0.499274\n",
      "epoch: 22 - cost: 3.71157 -MSE: 87.8676531546 -train accuracy: 0.501089\n",
      "epoch: 23 - cost: 3.69526 -MSE: 106.37691139 -train accuracy: 0.499274\n",
      "epoch: 24 - cost: 3.69287 -MSE: 86.5265340518 -train accuracy: 0.501089\n",
      "epoch: 25 - cost: 3.59616 -MSE: 104.053496238 -train accuracy: 0.499274\n",
      "epoch: 26 - cost: 3.67277 -MSE: 85.3576356601 -train accuracy: 0.50127\n",
      "epoch: 27 - cost: 3.50542 -MSE: 101.985990734 -train accuracy: 0.499274\n",
      "epoch: 28 - cost: 3.65175 -MSE: 84.3096018507 -train accuracy: 0.501451\n",
      "epoch: 29 - cost: 3.42065 -MSE: 100.103464032 -train accuracy: 0.499274\n",
      "epoch: 30 - cost: 3.63093 -MSE: 83.3666834263 -train accuracy: 0.501451\n",
      "epoch: 31 - cost: 3.34051 -MSE: 98.3836965133 -train accuracy: 0.499274\n",
      "epoch: 32 - cost: 3.61027 -MSE: 82.4857106705 -train accuracy: 0.501451\n",
      "epoch: 33 - cost: 3.26376 -MSE: 96.8398651162 -train accuracy: 0.499274\n",
      "epoch: 34 - cost: 3.58924 -MSE: 81.7008020579 -train accuracy: 0.501451\n",
      "epoch: 35 - cost: 3.18706 -MSE: 95.3922968266 -train accuracy: 0.499274\n",
      "epoch: 36 - cost: 3.56915 -MSE: 80.912115577 -train accuracy: 0.501451\n",
      "epoch: 37 - cost: 3.1071 -MSE: 94.0639250632 -train accuracy: 0.499274\n",
      "epoch: 38 - cost: 3.5494 -MSE: 80.1961705319 -train accuracy: 0.501451\n",
      "epoch: 39 - cost: 3.02644 -MSE: 92.7962838788 -train accuracy: 0.499274\n",
      "epoch: 40 - cost: 3.52439 -MSE: 79.3725028892 -train accuracy: 0.501451\n",
      "epoch: 41 - cost: 2.95173 -MSE: 91.6031843936 -train accuracy: 0.499093\n",
      "epoch: 42 - cost: 3.48991 -MSE: 78.5736593554 -train accuracy: 0.501451\n",
      "epoch: 43 - cost: 2.89058 -MSE: 90.4711240113 -train accuracy: 0.49873\n",
      "epoch: 44 - cost: 3.44785 -MSE: 77.8471441771 -train accuracy: 0.501451\n",
      "epoch: 45 - cost: 2.8456 -MSE: 89.5620106165 -train accuracy: 0.49873\n",
      "epoch: 46 - cost: 3.40176 -MSE: 77.1969368447 -train accuracy: 0.501451\n",
      "epoch: 47 - cost: 2.8075 -MSE: 88.835532056 -train accuracy: 0.49873\n",
      "epoch: 48 - cost: 3.35578 -MSE: 76.644720527 -train accuracy: 0.501451\n",
      "epoch: 49 - cost: 2.77243 -MSE: 88.2409582635 -train accuracy: 0.498911\n",
      "epoch: 50 - cost: 3.30931 -MSE: 76.1769057113 -train accuracy: 0.501451\n",
      "epoch: 51 - cost: 2.74016 -MSE: 87.7607924833 -train accuracy: 0.499093\n",
      "epoch: 52 - cost: 3.26092 -MSE: 75.7536070648 -train accuracy: 0.501451\n",
      "epoch: 53 - cost: 2.71149 -MSE: 87.3757003679 -train accuracy: 0.499093\n",
      "epoch: 54 - cost: 3.21189 -MSE: 75.4633835318 -train accuracy: 0.501451\n",
      "epoch: 55 - cost: 2.6861 -MSE: 87.0980714046 -train accuracy: 0.499093\n",
      "epoch: 56 - cost: 3.15991 -MSE: 75.2903548251 -train accuracy: 0.501451\n",
      "epoch: 57 - cost: 2.66667 -MSE: 86.8373955373 -train accuracy: 0.499093\n",
      "epoch: 58 - cost: 3.1082 -MSE: 75.189291739 -train accuracy: 0.501451\n",
      "epoch: 59 - cost: 2.64893 -MSE: 86.5497237075 -train accuracy: 0.499093\n",
      "epoch: 60 - cost: 3.05807 -MSE: 75.1134955985 -train accuracy: 0.501451\n",
      "epoch: 61 - cost: 2.63107 -MSE: 86.1895260107 -train accuracy: 0.499093\n",
      "epoch: 62 - cost: 3.01271 -MSE: 75.009379256 -train accuracy: 0.501451\n",
      "epoch: 63 - cost: 2.61057 -MSE: 85.7047415338 -train accuracy: 0.499093\n",
      "epoch: 64 - cost: 2.97271 -MSE: 74.8656170981 -train accuracy: 0.501451\n",
      "epoch: 65 - cost: 2.5877 -MSE: 85.0677081975 -train accuracy: 0.499093\n",
      "epoch: 66 - cost: 2.93887 -MSE: 74.4859744502 -train accuracy: 0.501451\n",
      "epoch: 67 - cost: 2.56494 -MSE: 84.2603956354 -train accuracy: 0.499093\n",
      "epoch: 68 - cost: 2.90954 -MSE: 73.9208611553 -train accuracy: 0.501451\n",
      "epoch: 69 - cost: 2.54094 -MSE: 83.3970957643 -train accuracy: 0.499093\n",
      "epoch: 70 - cost: 2.88218 -MSE: 73.2951137156 -train accuracy: 0.501451\n",
      "epoch: 71 - cost: 2.51734 -MSE: 82.5463147776 -train accuracy: 0.499093\n",
      "epoch: 72 - cost: 2.85623 -MSE: 72.6552492708 -train accuracy: 0.501451\n",
      "epoch: 73 - cost: 2.49375 -MSE: 81.704164645 -train accuracy: 0.499093\n",
      "epoch: 74 - cost: 2.83078 -MSE: 72.0151658912 -train accuracy: 0.501451\n",
      "epoch: 75 - cost: 2.46995 -MSE: 80.8451612716 -train accuracy: 0.499093\n",
      "epoch: 76 - cost: 2.80575 -MSE: 71.3456872882 -train accuracy: 0.501451\n",
      "epoch: 77 - cost: 2.44696 -MSE: 79.9565664335 -train accuracy: 0.499093\n",
      "epoch: 78 - cost: 2.78081 -MSE: 70.611630133 -train accuracy: 0.501451\n",
      "epoch: 79 - cost: 2.42509 -MSE: 79.0531925465 -train accuracy: 0.499093\n",
      "epoch: 80 - cost: 2.75611 -MSE: 69.8631719143 -train accuracy: 0.501451\n",
      "epoch: 81 - cost: 2.40389 -MSE: 78.1630597498 -train accuracy: 0.499093\n",
      "epoch: 82 - cost: 2.73163 -MSE: 69.1138140582 -train accuracy: 0.501451\n",
      "epoch: 83 - cost: 2.38304 -MSE: 77.2604182125 -train accuracy: 0.499093\n",
      "epoch: 84 - cost: 2.7072 -MSE: 68.349527956 -train accuracy: 0.501451\n",
      "epoch: 85 - cost: 2.36265 -MSE: 76.3611024042 -train accuracy: 0.499093\n",
      "epoch: 86 - cost: 2.68268 -MSE: 67.5807904737 -train accuracy: 0.501451\n",
      "epoch: 87 - cost: 2.34275 -MSE: 75.4663697614 -train accuracy: 0.499093\n",
      "epoch: 88 - cost: 2.6585 -MSE: 66.7979456843 -train accuracy: 0.501451\n",
      "epoch: 89 - cost: 2.32364 -MSE: 74.5739979993 -train accuracy: 0.499274\n",
      "epoch: 90 - cost: 2.63414 -MSE: 66.0330894604 -train accuracy: 0.501451\n",
      "epoch: 91 - cost: 2.30493 -MSE: 73.698233473 -train accuracy: 0.499274\n",
      "epoch: 92 - cost: 2.60977 -MSE: 65.2716840505 -train accuracy: 0.501451\n",
      "epoch: 93 - cost: 2.28648 -MSE: 72.8184179746 -train accuracy: 0.499274\n",
      "epoch: 94 - cost: 2.58574 -MSE: 64.5087459476 -train accuracy: 0.501451\n",
      "epoch: 95 - cost: 2.26801 -MSE: 71.939390296 -train accuracy: 0.499274\n",
      "epoch: 96 - cost: 2.56165 -MSE: 63.760370692 -train accuracy: 0.501451\n",
      "epoch: 97 - cost: 2.2499 -MSE: 71.0743414891 -train accuracy: 0.499274\n",
      "epoch: 98 - cost: 2.53754 -MSE: 63.0215908562 -train accuracy: 0.501451\n",
      "epoch: 99 - cost: 2.23215 -MSE: 70.2150663892 -train accuracy: 0.499274\n",
      "epoch: 100 - cost: 2.51312 -MSE: 62.277343534 -train accuracy: 0.501451\n",
      "epoch: 101 - cost: 2.21521 -MSE: 69.3597701457 -train accuracy: 0.499274\n",
      "epoch: 102 - cost: 2.48903 -MSE: 61.525160718 -train accuracy: 0.501451\n",
      "epoch: 103 - cost: 2.1983 -MSE: 68.5080457149 -train accuracy: 0.499274\n",
      "epoch: 104 - cost: 2.46491 -MSE: 60.7944782076 -train accuracy: 0.501451\n",
      "epoch: 105 - cost: 2.18243 -MSE: 67.6865955868 -train accuracy: 0.499274\n",
      "epoch: 106 - cost: 2.44089 -MSE: 60.0796006138 -train accuracy: 0.501451\n",
      "epoch: 107 - cost: 2.16689 -MSE: 66.8854381163 -train accuracy: 0.499274\n",
      "epoch: 108 - cost: 2.41712 -MSE: 59.3851244012 -train accuracy: 0.501451\n",
      "epoch: 109 - cost: 2.15182 -MSE: 66.1068556644 -train accuracy: 0.499274\n",
      "epoch: 110 - cost: 2.39335 -MSE: 58.7052331315 -train accuracy: 0.501451\n",
      "epoch: 111 - cost: 2.13737 -MSE: 65.3432271211 -train accuracy: 0.499274\n",
      "epoch: 112 - cost: 2.36979 -MSE: 58.0411546566 -train accuracy: 0.501451\n",
      "epoch: 113 - cost: 2.1233 -MSE: 64.5904411473 -train accuracy: 0.499274\n",
      "epoch: 114 - cost: 2.34678 -MSE: 57.3813093725 -train accuracy: 0.501451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 115 - cost: 2.10923 -MSE: 63.8499513806 -train accuracy: 0.499274\n",
      "epoch: 116 - cost: 2.32434 -MSE: 56.7372436834 -train accuracy: 0.501451\n",
      "epoch: 117 - cost: 2.09523 -MSE: 63.1295729545 -train accuracy: 0.499274\n",
      "epoch: 118 - cost: 2.30265 -MSE: 56.1083473452 -train accuracy: 0.501451\n",
      "epoch: 119 - cost: 2.08132 -MSE: 62.4213654258 -train accuracy: 0.499274\n",
      "epoch: 120 - cost: 2.28136 -MSE: 55.4935070613 -train accuracy: 0.501451\n",
      "epoch: 121 - cost: 2.06759 -MSE: 61.7233786951 -train accuracy: 0.499274\n",
      "epoch: 122 - cost: 2.2605 -MSE: 54.8908386643 -train accuracy: 0.501451\n",
      "epoch: 123 - cost: 2.05417 -MSE: 61.0425435146 -train accuracy: 0.499274\n",
      "epoch: 124 - cost: 2.24019 -MSE: 54.300125189 -train accuracy: 0.50127\n",
      "epoch: 125 - cost: 2.04078 -MSE: 60.3689089939 -train accuracy: 0.499274\n",
      "epoch: 126 - cost: 2.22034 -MSE: 53.711980537 -train accuracy: 0.50127\n",
      "epoch: 127 - cost: 2.02752 -MSE: 59.7020890942 -train accuracy: 0.499274\n",
      "epoch: 128 - cost: 2.20086 -MSE: 53.1321408237 -train accuracy: 0.501089\n",
      "epoch: 129 - cost: 2.01441 -MSE: 59.0497253487 -train accuracy: 0.499274\n",
      "epoch: 130 - cost: 2.1819 -MSE: 52.5652079714 -train accuracy: 0.501089\n",
      "epoch: 131 - cost: 2.00149 -MSE: 58.4117710401 -train accuracy: 0.499274\n",
      "epoch: 132 - cost: 2.16336 -MSE: 52.0105256895 -train accuracy: 0.501089\n",
      "epoch: 133 - cost: 1.98843 -MSE: 57.7841889128 -train accuracy: 0.499274\n",
      "epoch: 134 - cost: 2.14535 -MSE: 51.4697549123 -train accuracy: 0.501089\n",
      "epoch: 135 - cost: 1.97566 -MSE: 57.1761738487 -train accuracy: 0.499274\n",
      "epoch: 136 - cost: 2.12768 -MSE: 50.9424899798 -train accuracy: 0.501814\n",
      "epoch: 137 - cost: 1.96309 -MSE: 56.579829322 -train accuracy: 0.499274\n",
      "epoch: 138 - cost: 2.11036 -MSE: 50.4229369393 -train accuracy: 0.501996\n",
      "epoch: 139 - cost: 1.9505 -MSE: 55.9958347066 -train accuracy: 0.499274\n",
      "epoch: 140 - cost: 2.09347 -MSE: 49.91672012 -train accuracy: 0.50254\n",
      "epoch: 141 - cost: 1.938 -MSE: 55.4231514497 -train accuracy: 0.499274\n",
      "epoch: 142 - cost: 2.07669 -MSE: 49.4191063079 -train accuracy: 0.502721\n",
      "epoch: 143 - cost: 1.9259 -MSE: 54.8612154833 -train accuracy: 0.499274\n",
      "epoch: 144 - cost: 2.06004 -MSE: 48.9275244078 -train accuracy: 0.502721\n",
      "epoch: 145 - cost: 1.91402 -MSE: 54.3100191203 -train accuracy: 0.499274\n",
      "epoch: 146 - cost: 2.04395 -MSE: 48.4458283879 -train accuracy: 0.503084\n",
      "epoch: 147 - cost: 1.90213 -MSE: 53.7692370048 -train accuracy: 0.499274\n",
      "epoch: 148 - cost: 2.02821 -MSE: 47.9745861302 -train accuracy: 0.503084\n",
      "epoch: 149 - cost: 1.89038 -MSE: 53.2417708381 -train accuracy: 0.499456\n",
      "epoch: 150 - cost: 2.01268 -MSE: 47.5148373166 -train accuracy: 0.503084\n",
      "epoch: 151 - cost: 1.8789 -MSE: 52.7241790236 -train accuracy: 0.499456\n",
      "epoch: 152 - cost: 1.9976 -MSE: 47.0590063464 -train accuracy: 0.503266\n",
      "epoch: 153 - cost: 1.86728 -MSE: 52.2101789858 -train accuracy: 0.499274\n",
      "epoch: 154 - cost: 1.98271 -MSE: 46.6097452163 -train accuracy: 0.503266\n",
      "epoch: 155 - cost: 1.85586 -MSE: 51.7024851961 -train accuracy: 0.499274\n",
      "epoch: 156 - cost: 1.96814 -MSE: 46.1637059866 -train accuracy: 0.503084\n",
      "epoch: 157 - cost: 1.8446 -MSE: 51.2022609745 -train accuracy: 0.499274\n",
      "epoch: 158 - cost: 1.95375 -MSE: 45.7227749364 -train accuracy: 0.503084\n",
      "epoch: 159 - cost: 1.83336 -MSE: 50.7063356127 -train accuracy: 0.499456\n",
      "epoch: 160 - cost: 1.93946 -MSE: 45.2889269338 -train accuracy: 0.503084\n",
      "epoch: 161 - cost: 1.82233 -MSE: 50.2202260618 -train accuracy: 0.499456\n",
      "epoch: 162 - cost: 1.92551 -MSE: 44.8623264468 -train accuracy: 0.503266\n",
      "epoch: 163 - cost: 1.81131 -MSE: 49.7424028262 -train accuracy: 0.499274\n",
      "epoch: 164 - cost: 1.91191 -MSE: 44.4428952901 -train accuracy: 0.503447\n",
      "epoch: 165 - cost: 1.8003 -MSE: 49.2693725057 -train accuracy: 0.499093\n",
      "epoch: 166 - cost: 1.89853 -MSE: 44.0285512647 -train accuracy: 0.503628\n",
      "epoch: 167 - cost: 1.78937 -MSE: 48.8030571912 -train accuracy: 0.499093\n",
      "epoch: 168 - cost: 1.88521 -MSE: 43.6189250711 -train accuracy: 0.503991\n",
      "epoch: 169 - cost: 1.7786 -MSE: 48.3428273881 -train accuracy: 0.499274\n",
      "epoch: 170 - cost: 1.87207 -MSE: 43.2146219806 -train accuracy: 0.504173\n",
      "epoch: 171 - cost: 1.76789 -MSE: 47.889381601 -train accuracy: 0.499274\n",
      "epoch: 172 - cost: 1.85905 -MSE: 42.8146203766 -train accuracy: 0.504173\n",
      "epoch: 173 - cost: 1.75732 -MSE: 47.4407226359 -train accuracy: 0.499274\n",
      "epoch: 174 - cost: 1.84624 -MSE: 42.4177030168 -train accuracy: 0.504173\n",
      "epoch: 175 - cost: 1.74685 -MSE: 46.9965848909 -train accuracy: 0.499456\n",
      "epoch: 176 - cost: 1.83365 -MSE: 42.0248480193 -train accuracy: 0.504173\n",
      "epoch: 177 - cost: 1.7364 -MSE: 46.5549654269 -train accuracy: 0.499456\n",
      "epoch: 178 - cost: 1.82114 -MSE: 41.6347913708 -train accuracy: 0.504173\n",
      "epoch: 179 - cost: 1.7261 -MSE: 46.1204301147 -train accuracy: 0.499456\n",
      "epoch: 180 - cost: 1.80881 -MSE: 41.252421213 -train accuracy: 0.504173\n",
      "epoch: 181 - cost: 1.71583 -MSE: 45.6916126187 -train accuracy: 0.499456\n",
      "epoch: 182 - cost: 1.79662 -MSE: 40.8741764213 -train accuracy: 0.504898\n",
      "epoch: 183 - cost: 1.70569 -MSE: 45.2680263319 -train accuracy: 0.499456\n",
      "epoch: 184 - cost: 1.78454 -MSE: 40.4982446451 -train accuracy: 0.504898\n",
      "epoch: 185 - cost: 1.69563 -MSE: 44.8493148452 -train accuracy: 0.499819\n",
      "epoch: 186 - cost: 1.77254 -MSE: 40.1281732149 -train accuracy: 0.50508\n",
      "epoch: 187 - cost: 1.68563 -MSE: 44.4361212396 -train accuracy: 0.499819\n",
      "epoch: 188 - cost: 1.76067 -MSE: 39.760696874 -train accuracy: 0.505261\n",
      "epoch: 189 - cost: 1.67578 -MSE: 44.0257095897 -train accuracy: 0.499819\n",
      "epoch: 190 - cost: 1.74902 -MSE: 39.3973930705 -train accuracy: 0.505261\n",
      "epoch: 191 - cost: 1.66601 -MSE: 43.6209281138 -train accuracy: 0.5\n",
      "epoch: 192 - cost: 1.73746 -MSE: 39.0373453874 -train accuracy: 0.505261\n",
      "epoch: 193 - cost: 1.65621 -MSE: 43.2206645668 -train accuracy: 0.5\n",
      "epoch: 194 - cost: 1.72603 -MSE: 38.6828948971 -train accuracy: 0.505261\n",
      "epoch: 195 - cost: 1.64649 -MSE: 42.8258187518 -train accuracy: 0.5\n",
      "epoch: 196 - cost: 1.71469 -MSE: 38.3330285432 -train accuracy: 0.505805\n",
      "epoch: 197 - cost: 1.63684 -MSE: 42.4363183127 -train accuracy: 0.500181\n",
      "epoch: 198 - cost: 1.70347 -MSE: 37.9861731925 -train accuracy: 0.505443\n",
      "epoch: 199 - cost: 1.62732 -MSE: 42.0501336135 -train accuracy: 0.500181\n",
      "epoch: 200 - cost: 1.69233 -MSE: 37.6415931877 -train accuracy: 0.505443\n",
      "epoch: 201 - cost: 1.61783 -MSE: 41.665993026 -train accuracy: 0.500544\n",
      "epoch: 202 - cost: 1.68135 -MSE: 37.3009174853 -train accuracy: 0.505624\n",
      "epoch: 203 - cost: 1.60845 -MSE: 41.2864030337 -train accuracy: 0.500726\n",
      "epoch: 204 - cost: 1.67038 -MSE: 36.9631251227 -train accuracy: 0.505987\n",
      "epoch: 205 - cost: 1.59922 -MSE: 40.909504262 -train accuracy: 0.500726\n",
      "epoch: 206 - cost: 1.6596 -MSE: 36.6264437846 -train accuracy: 0.506168\n",
      "epoch: 207 - cost: 1.58992 -MSE: 40.5350932839 -train accuracy: 0.500544\n",
      "epoch: 208 - cost: 1.649 -MSE: 36.2933754989 -train accuracy: 0.506168\n",
      "epoch: 209 - cost: 1.58073 -MSE: 40.1644246046 -train accuracy: 0.500726\n",
      "epoch: 210 - cost: 1.63839 -MSE: 35.9624290515 -train accuracy: 0.506168\n",
      "epoch: 211 - cost: 1.57163 -MSE: 39.7970922401 -train accuracy: 0.500544\n",
      "epoch: 212 - cost: 1.6278 -MSE: 35.633947825 -train accuracy: 0.506168\n",
      "epoch: 213 - cost: 1.56259 -MSE: 39.431334328 -train accuracy: 0.500544\n",
      "epoch: 214 - cost: 1.61745 -MSE: 35.307655446 -train accuracy: 0.506894\n",
      "epoch: 215 - cost: 1.55355 -MSE: 39.06908262 -train accuracy: 0.500363\n",
      "epoch: 216 - cost: 1.60709 -MSE: 34.9864954504 -train accuracy: 0.507257\n",
      "epoch: 217 - cost: 1.54462 -MSE: 38.7110122147 -train accuracy: 0.500544\n",
      "epoch: 218 - cost: 1.59683 -MSE: 34.6666790102 -train accuracy: 0.507438\n",
      "epoch: 219 - cost: 1.53576 -MSE: 38.3561794872 -train accuracy: 0.500544\n",
      "epoch: 220 - cost: 1.5867 -MSE: 34.3508899189 -train accuracy: 0.50762\n",
      "epoch: 221 - cost: 1.52695 -MSE: 38.0055473548 -train accuracy: 0.500544\n",
      "epoch: 222 - cost: 1.57663 -MSE: 34.0382406223 -train accuracy: 0.50762\n",
      "epoch: 223 - cost: 1.51813 -MSE: 37.6578794316 -train accuracy: 0.500544\n",
      "epoch: 224 - cost: 1.56667 -MSE: 33.7285033299 -train accuracy: 0.50762\n",
      "epoch: 225 - cost: 1.50933 -MSE: 37.313790756 -train accuracy: 0.500726\n",
      "epoch: 226 - cost: 1.55683 -MSE: 33.4227568139 -train accuracy: 0.507801\n",
      "epoch: 227 - cost: 1.50066 -MSE: 36.9730691934 -train accuracy: 0.500726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 228 - cost: 1.54697 -MSE: 33.1190658372 -train accuracy: 0.508527\n",
      "epoch: 229 - cost: 1.49213 -MSE: 36.6349396615 -train accuracy: 0.500544\n",
      "epoch: 230 - cost: 1.5372 -MSE: 32.8165764152 -train accuracy: 0.50889\n",
      "epoch: 231 - cost: 1.48362 -MSE: 36.2986432481 -train accuracy: 0.500726\n",
      "epoch: 232 - cost: 1.52751 -MSE: 32.5164106358 -train accuracy: 0.50889\n",
      "epoch: 233 - cost: 1.47514 -MSE: 35.9648520649 -train accuracy: 0.500544\n",
      "epoch: 234 - cost: 1.51798 -MSE: 32.218810364 -train accuracy: 0.509434\n",
      "epoch: 235 - cost: 1.46669 -MSE: 35.6339881487 -train accuracy: 0.500907\n",
      "epoch: 236 - cost: 1.50855 -MSE: 31.9239466027 -train accuracy: 0.509615\n",
      "epoch: 237 - cost: 1.45831 -MSE: 35.3056254848 -train accuracy: 0.500907\n",
      "epoch: 238 - cost: 1.49914 -MSE: 31.6324047831 -train accuracy: 0.509978\n",
      "epoch: 239 - cost: 1.45007 -MSE: 34.9812393434 -train accuracy: 0.501089\n",
      "epoch: 240 - cost: 1.48976 -MSE: 31.3412918531 -train accuracy: 0.509797\n",
      "epoch: 241 - cost: 1.44188 -MSE: 34.6587089054 -train accuracy: 0.500907\n",
      "epoch: 242 - cost: 1.48049 -MSE: 31.0527649397 -train accuracy: 0.509797\n",
      "epoch: 243 - cost: 1.43365 -MSE: 34.3386016794 -train accuracy: 0.500544\n",
      "epoch: 244 - cost: 1.47132 -MSE: 30.7668922405 -train accuracy: 0.510341\n",
      "epoch: 245 - cost: 1.42546 -MSE: 34.0217911802 -train accuracy: 0.500544\n",
      "epoch: 246 - cost: 1.46219 -MSE: 30.4844773534 -train accuracy: 0.510341\n",
      "epoch: 247 - cost: 1.41733 -MSE: 33.7087398071 -train accuracy: 0.501089\n",
      "epoch: 248 - cost: 1.45323 -MSE: 30.2065191123 -train accuracy: 0.509978\n",
      "epoch: 249 - cost: 1.40925 -MSE: 33.3990821145 -train accuracy: 0.501089\n",
      "epoch: 250 - cost: 1.44428 -MSE: 29.9316801631 -train accuracy: 0.51016\n",
      "epoch: 251 - cost: 1.40122 -MSE: 33.0929017928 -train accuracy: 0.501451\n",
      "epoch: 252 - cost: 1.43542 -MSE: 29.6604313656 -train accuracy: 0.510522\n",
      "epoch: 253 - cost: 1.39321 -MSE: 32.7891152664 -train accuracy: 0.501633\n",
      "epoch: 254 - cost: 1.42665 -MSE: 29.3911257253 -train accuracy: 0.510704\n",
      "epoch: 255 - cost: 1.38532 -MSE: 32.4882467036 -train accuracy: 0.501996\n",
      "epoch: 256 - cost: 1.41789 -MSE: 29.1226826671 -train accuracy: 0.510704\n",
      "epoch: 257 - cost: 1.3776 -MSE: 32.1891113633 -train accuracy: 0.502177\n",
      "epoch: 258 - cost: 1.4092 -MSE: 28.8563119924 -train accuracy: 0.51143\n",
      "epoch: 259 - cost: 1.36979 -MSE: 31.8919494465 -train accuracy: 0.502358\n",
      "epoch: 260 - cost: 1.40068 -MSE: 28.5935735809 -train accuracy: 0.511611\n",
      "epoch: 261 - cost: 1.36205 -MSE: 31.5969948888 -train accuracy: 0.50254\n",
      "epoch: 262 - cost: 1.39219 -MSE: 28.3315274036 -train accuracy: 0.511611\n",
      "epoch: 263 - cost: 1.35443 -MSE: 31.303672346 -train accuracy: 0.502721\n",
      "epoch: 264 - cost: 1.38375 -MSE: 28.0715523369 -train accuracy: 0.511974\n",
      "epoch: 265 - cost: 1.34681 -MSE: 31.0138386633 -train accuracy: 0.502721\n",
      "epoch: 266 - cost: 1.37541 -MSE: 27.8137950943 -train accuracy: 0.512337\n",
      "epoch: 267 - cost: 1.33925 -MSE: 30.7272665083 -train accuracy: 0.502903\n",
      "epoch: 268 - cost: 1.36701 -MSE: 27.559502583 -train accuracy: 0.512518\n",
      "epoch: 269 - cost: 1.33166 -MSE: 30.4425678731 -train accuracy: 0.503447\n",
      "epoch: 270 - cost: 1.35875 -MSE: 27.3069139324 -train accuracy: 0.512337\n",
      "epoch: 271 - cost: 1.32421 -MSE: 30.1591008995 -train accuracy: 0.503447\n",
      "epoch: 272 - cost: 1.35066 -MSE: 27.0566461803 -train accuracy: 0.5127\n",
      "epoch: 273 - cost: 1.31684 -MSE: 29.8786448202 -train accuracy: 0.503447\n",
      "epoch: 274 - cost: 1.34244 -MSE: 26.8050163029 -train accuracy: 0.513244\n",
      "epoch: 275 - cost: 1.30949 -MSE: 29.5979601081 -train accuracy: 0.503991\n",
      "epoch: 276 - cost: 1.33433 -MSE: 26.5528974496 -train accuracy: 0.513607\n",
      "epoch: 277 - cost: 1.30217 -MSE: 29.3185012 -train accuracy: 0.504717\n",
      "epoch: 278 - cost: 1.3263 -MSE: 26.3033641432 -train accuracy: 0.513607\n",
      "epoch: 279 - cost: 1.29495 -MSE: 29.0406351246 -train accuracy: 0.504898\n",
      "epoch: 280 - cost: 1.31839 -MSE: 26.0563230349 -train accuracy: 0.513607\n",
      "epoch: 281 - cost: 1.28776 -MSE: 28.7661268521 -train accuracy: 0.50508\n",
      "epoch: 282 - cost: 1.31059 -MSE: 25.8124801228 -train accuracy: 0.513425\n",
      "epoch: 283 - cost: 1.28063 -MSE: 28.4937392263 -train accuracy: 0.505443\n",
      "epoch: 284 - cost: 1.30285 -MSE: 25.570390148 -train accuracy: 0.51397\n",
      "epoch: 285 - cost: 1.27348 -MSE: 28.222812585 -train accuracy: 0.505261\n",
      "epoch: 286 - cost: 1.29517 -MSE: 25.3304163932 -train accuracy: 0.514695\n",
      "epoch: 287 - cost: 1.26646 -MSE: 27.9547184802 -train accuracy: 0.505443\n",
      "epoch: 288 - cost: 1.28759 -MSE: 25.0938501987 -train accuracy: 0.514877\n",
      "epoch: 289 - cost: 1.2595 -MSE: 27.6901969179 -train accuracy: 0.505805\n",
      "epoch: 290 - cost: 1.27995 -MSE: 24.8588516587 -train accuracy: 0.515239\n",
      "epoch: 291 - cost: 1.25258 -MSE: 27.4275316499 -train accuracy: 0.50635\n",
      "epoch: 292 - cost: 1.27247 -MSE: 24.6269588592 -train accuracy: 0.515421\n",
      "epoch: 293 - cost: 1.24573 -MSE: 27.16785272 -train accuracy: 0.507075\n",
      "epoch: 294 - cost: 1.26507 -MSE: 24.3963130849 -train accuracy: 0.515965\n",
      "epoch: 295 - cost: 1.23888 -MSE: 26.9112002468 -train accuracy: 0.507257\n",
      "epoch: 296 - cost: 1.25768 -MSE: 24.1691328348 -train accuracy: 0.516328\n",
      "epoch: 297 - cost: 1.23208 -MSE: 26.6579084616 -train accuracy: 0.50762\n",
      "epoch: 298 - cost: 1.25026 -MSE: 23.941167355 -train accuracy: 0.516509\n",
      "epoch: 299 - cost: 1.2253 -MSE: 26.4037516358 -train accuracy: 0.508164\n",
      "epoch: 300 - cost: 1.24306 -MSE: 23.7156671056 -train accuracy: 0.516872\n",
      "epoch: 301 - cost: 1.21861 -MSE: 26.151494414 -train accuracy: 0.50889\n",
      "epoch: 302 - cost: 1.23592 -MSE: 23.4915521345 -train accuracy: 0.516872\n",
      "epoch: 303 - cost: 1.21201 -MSE: 25.9003874512 -train accuracy: 0.509434\n",
      "epoch: 304 - cost: 1.22881 -MSE: 23.2688259135 -train accuracy: 0.516872\n",
      "epoch: 305 - cost: 1.20541 -MSE: 25.6509522698 -train accuracy: 0.509797\n",
      "epoch: 306 - cost: 1.22176 -MSE: 23.0450755401 -train accuracy: 0.517054\n",
      "epoch: 307 - cost: 1.19884 -MSE: 25.4026586844 -train accuracy: 0.510522\n",
      "epoch: 308 - cost: 1.2148 -MSE: 22.82551203 -train accuracy: 0.517054\n",
      "epoch: 309 - cost: 1.19242 -MSE: 25.1571368518 -train accuracy: 0.51143\n",
      "epoch: 310 - cost: 1.20792 -MSE: 22.6086279151 -train accuracy: 0.517054\n",
      "epoch: 311 - cost: 1.18598 -MSE: 24.9144982149 -train accuracy: 0.511792\n",
      "epoch: 312 - cost: 1.20105 -MSE: 22.392138172 -train accuracy: 0.517054\n",
      "epoch: 313 - cost: 1.17958 -MSE: 24.6737098104 -train accuracy: 0.511974\n",
      "epoch: 314 - cost: 1.19421 -MSE: 22.1793152425 -train accuracy: 0.517417\n",
      "epoch: 315 - cost: 1.17327 -MSE: 24.4366766825 -train accuracy: 0.512881\n",
      "epoch: 316 - cost: 1.1874 -MSE: 21.9663509069 -train accuracy: 0.517779\n",
      "epoch: 317 - cost: 1.16704 -MSE: 24.1986161705 -train accuracy: 0.5127\n",
      "epoch: 318 - cost: 1.18075 -MSE: 21.7547421172 -train accuracy: 0.517961\n",
      "epoch: 319 - cost: 1.16084 -MSE: 23.9609854 -train accuracy: 0.513062\n",
      "epoch: 320 - cost: 1.1742 -MSE: 21.542433573 -train accuracy: 0.517598\n",
      "epoch: 321 - cost: 1.15467 -MSE: 23.725659575 -train accuracy: 0.513788\n",
      "epoch: 322 - cost: 1.16759 -MSE: 21.3315282065 -train accuracy: 0.517961\n",
      "epoch: 323 - cost: 1.14844 -MSE: 23.4921619745 -train accuracy: 0.514514\n",
      "epoch: 324 - cost: 1.161 -MSE: 21.121611727 -train accuracy: 0.518142\n",
      "epoch: 325 - cost: 1.14233 -MSE: 23.2601949224 -train accuracy: 0.514877\n",
      "epoch: 326 - cost: 1.15455 -MSE: 20.9150267398 -train accuracy: 0.518868\n",
      "epoch: 327 - cost: 1.13631 -MSE: 23.0301662926 -train accuracy: 0.515421\n",
      "epoch: 328 - cost: 1.14821 -MSE: 20.7083198029 -train accuracy: 0.519231\n",
      "epoch: 329 - cost: 1.13039 -MSE: 22.8006912051 -train accuracy: 0.515965\n",
      "epoch: 330 - cost: 1.14197 -MSE: 20.5032425277 -train accuracy: 0.519775\n",
      "epoch: 331 - cost: 1.12447 -MSE: 22.5757616791 -train accuracy: 0.516147\n",
      "epoch: 332 - cost: 1.1358 -MSE: 20.3032481909 -train accuracy: 0.520319\n",
      "epoch: 333 - cost: 1.11873 -MSE: 22.352057837 -train accuracy: 0.516509\n",
      "epoch: 334 - cost: 1.1296 -MSE: 20.1012983269 -train accuracy: 0.521589\n",
      "epoch: 335 - cost: 1.11293 -MSE: 22.1310591446 -train accuracy: 0.517598\n",
      "epoch: 336 - cost: 1.1234 -MSE: 19.9011606723 -train accuracy: 0.523403\n",
      "epoch: 337 - cost: 1.10716 -MSE: 21.9099277998 -train accuracy: 0.517779\n",
      "epoch: 338 - cost: 1.11733 -MSE: 19.7007197953 -train accuracy: 0.524311\n",
      "epoch: 339 - cost: 1.10151 -MSE: 21.6877958885 -train accuracy: 0.518505\n",
      "epoch: 340 - cost: 1.11139 -MSE: 19.5012136788 -train accuracy: 0.525036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 341 - cost: 1.09583 -MSE: 21.4692145165 -train accuracy: 0.519049\n",
      "epoch: 342 - cost: 1.10546 -MSE: 19.3048093912 -train accuracy: 0.525218\n",
      "epoch: 343 - cost: 1.09035 -MSE: 21.2513514136 -train accuracy: 0.519775\n",
      "epoch: 344 - cost: 1.09966 -MSE: 19.1091020411 -train accuracy: 0.525399\n",
      "epoch: 345 - cost: 1.08488 -MSE: 21.0353669121 -train accuracy: 0.519775\n",
      "epoch: 346 - cost: 1.09393 -MSE: 18.9161030216 -train accuracy: 0.525762\n",
      "epoch: 347 - cost: 1.07952 -MSE: 20.8214977763 -train accuracy: 0.519775\n",
      "epoch: 348 - cost: 1.08833 -MSE: 18.725301437 -train accuracy: 0.526306\n",
      "epoch: 349 - cost: 1.07411 -MSE: 20.6125009995 -train accuracy: 0.520682\n",
      "epoch: 350 - cost: 1.08264 -MSE: 18.5381324816 -train accuracy: 0.527213\n",
      "epoch: 351 - cost: 1.06883 -MSE: 20.4067537337 -train accuracy: 0.521589\n",
      "epoch: 352 - cost: 1.07697 -MSE: 18.3510938766 -train accuracy: 0.527758\n",
      "epoch: 353 - cost: 1.0635 -MSE: 20.2003287716 -train accuracy: 0.522496\n",
      "epoch: 354 - cost: 1.0714 -MSE: 18.1651901684 -train accuracy: 0.528665\n",
      "epoch: 355 - cost: 1.05821 -MSE: 19.9965329827 -train accuracy: 0.523222\n",
      "epoch: 356 - cost: 1.0659 -MSE: 17.9811681339 -train accuracy: 0.529209\n",
      "epoch: 357 - cost: 1.05304 -MSE: 19.7951196832 -train accuracy: 0.523948\n",
      "epoch: 358 - cost: 1.06051 -MSE: 17.7996877526 -train accuracy: 0.530116\n",
      "epoch: 359 - cost: 1.04791 -MSE: 19.5958723171 -train accuracy: 0.524129\n",
      "epoch: 360 - cost: 1.05513 -MSE: 17.6214201542 -train accuracy: 0.531386\n",
      "epoch: 361 - cost: 1.04269 -MSE: 19.4035348158 -train accuracy: 0.524855\n",
      "epoch: 362 - cost: 1.04979 -MSE: 17.4488922375 -train accuracy: 0.532112\n",
      "epoch: 363 - cost: 1.03769 -MSE: 19.2121371199 -train accuracy: 0.526851\n",
      "epoch: 364 - cost: 1.04452 -MSE: 17.2758864042 -train accuracy: 0.532293\n",
      "epoch: 365 - cost: 1.03267 -MSE: 19.0215506273 -train accuracy: 0.527395\n",
      "epoch: 366 - cost: 1.0393 -MSE: 17.1049101652 -train accuracy: 0.533926\n",
      "epoch: 367 - cost: 1.02773 -MSE: 18.8338030129 -train accuracy: 0.527939\n",
      "epoch: 368 - cost: 1.03408 -MSE: 16.9366612412 -train accuracy: 0.534833\n",
      "epoch: 369 - cost: 1.02264 -MSE: 18.651388883 -train accuracy: 0.528483\n",
      "epoch: 370 - cost: 1.02878 -MSE: 16.7702913376 -train accuracy: 0.536466\n",
      "epoch: 371 - cost: 1.01766 -MSE: 18.4689765527 -train accuracy: 0.52939\n",
      "epoch: 372 - cost: 1.02365 -MSE: 16.6068004585 -train accuracy: 0.537736\n",
      "epoch: 373 - cost: 1.01272 -MSE: 18.2917017443 -train accuracy: 0.530116\n",
      "epoch: 374 - cost: 1.01862 -MSE: 16.4486334688 -train accuracy: 0.538462\n",
      "epoch: 375 - cost: 1.00786 -MSE: 18.1187206371 -train accuracy: 0.530842\n",
      "epoch: 376 - cost: 1.01365 -MSE: 16.2963031442 -train accuracy: 0.539006\n",
      "epoch: 377 - cost: 1.00311 -MSE: 17.9495731992 -train accuracy: 0.531023\n",
      "epoch: 378 - cost: 1.00868 -MSE: 16.1435255566 -train accuracy: 0.539369\n",
      "epoch: 379 - cost: 0.998443 -MSE: 17.7779217543 -train accuracy: 0.532112\n",
      "epoch: 380 - cost: 1.00382 -MSE: 15.99001326 -train accuracy: 0.539369\n",
      "epoch: 381 - cost: 0.993735 -MSE: 17.6105658881 -train accuracy: 0.532837\n",
      "epoch: 382 - cost: 0.998991 -MSE: 15.8407099377 -train accuracy: 0.539913\n",
      "epoch: 383 - cost: 0.98916 -MSE: 17.4446317568 -train accuracy: 0.534107\n",
      "epoch: 384 - cost: 0.994268 -MSE: 15.6932865913 -train accuracy: 0.540639\n",
      "epoch: 385 - cost: 0.984599 -MSE: 17.2818919144 -train accuracy: 0.534833\n",
      "epoch: 386 - cost: 0.989531 -MSE: 15.5465632727 -train accuracy: 0.541183\n",
      "epoch: 387 - cost: 0.98008 -MSE: 17.1198757292 -train accuracy: 0.535196\n",
      "epoch: 388 - cost: 0.984833 -MSE: 15.4020171957 -train accuracy: 0.541364\n",
      "epoch: 389 - cost: 0.975568 -MSE: 16.9594125621 -train accuracy: 0.53701\n",
      "epoch: 390 - cost: 0.980123 -MSE: 15.2572445145 -train accuracy: 0.542271\n",
      "epoch: 391 - cost: 0.971137 -MSE: 16.7995858986 -train accuracy: 0.537192\n",
      "epoch: 392 - cost: 0.97552 -MSE: 15.1140250807 -train accuracy: 0.543723\n",
      "epoch: 393 - cost: 0.966834 -MSE: 16.639124108 -train accuracy: 0.537736\n",
      "epoch: 394 - cost: 0.971 -MSE: 14.9703127921 -train accuracy: 0.54463\n",
      "epoch: 395 - cost: 0.962483 -MSE: 16.480737289 -train accuracy: 0.538824\n",
      "epoch: 396 - cost: 0.966435 -MSE: 14.8282764895 -train accuracy: 0.545718\n",
      "epoch: 397 - cost: 0.958051 -MSE: 16.3264696904 -train accuracy: 0.539006\n",
      "epoch: 398 - cost: 0.961917 -MSE: 14.6910112105 -train accuracy: 0.546988\n",
      "epoch: 399 - cost: 0.953613 -MSE: 16.1783724861 -train accuracy: 0.54082\n",
      "epoch: 400 - cost: 0.957417 -MSE: 14.5601425511 -train accuracy: 0.548077\n",
      "epoch: 401 - cost: 0.949322 -MSE: 16.0341225223 -train accuracy: 0.541546\n",
      "epoch: 402 - cost: 0.953082 -MSE: 14.434013748 -train accuracy: 0.548803\n",
      "epoch: 403 - cost: 0.945198 -MSE: 15.8917321601 -train accuracy: 0.542997\n",
      "epoch: 404 - cost: 0.948804 -MSE: 14.3073289318 -train accuracy: 0.549891\n",
      "epoch: 405 - cost: 0.941006 -MSE: 15.7519056427 -train accuracy: 0.544448\n",
      "epoch: 406 - cost: 0.944449 -MSE: 14.1822503301 -train accuracy: 0.550254\n",
      "epoch: 407 - cost: 0.936761 -MSE: 15.6150527012 -train accuracy: 0.545174\n",
      "epoch: 408 - cost: 0.9401 -MSE: 14.0589192616 -train accuracy: 0.550617\n",
      "epoch: 409 - cost: 0.932609 -MSE: 15.4784074202 -train accuracy: 0.545174\n",
      "epoch: 410 - cost: 0.935888 -MSE: 13.9380858881 -train accuracy: 0.551343\n",
      "epoch: 411 - cost: 0.928552 -MSE: 15.3440228081 -train accuracy: 0.546988\n",
      "epoch: 412 - cost: 0.93177 -MSE: 13.8203491866 -train accuracy: 0.55225\n",
      "epoch: 413 - cost: 0.924537 -MSE: 15.212961423 -train accuracy: 0.548077\n",
      "epoch: 414 - cost: 0.927678 -MSE: 13.7051244754 -train accuracy: 0.552794\n",
      "epoch: 415 - cost: 0.920537 -MSE: 15.0854889998 -train accuracy: 0.548621\n",
      "epoch: 416 - cost: 0.923604 -MSE: 13.5917454989 -train accuracy: 0.553157\n",
      "epoch: 417 - cost: 0.916558 -MSE: 14.9609381211 -train accuracy: 0.549528\n",
      "epoch: 418 - cost: 0.91955 -MSE: 13.4826025317 -train accuracy: 0.554064\n",
      "epoch: 419 - cost: 0.912685 -MSE: 14.8388201844 -train accuracy: 0.550798\n",
      "epoch: 420 - cost: 0.915573 -MSE: 13.3740587959 -train accuracy: 0.55479\n",
      "epoch: 421 - cost: 0.908848 -MSE: 14.715526673 -train accuracy: 0.55225\n",
      "epoch: 422 - cost: 0.911669 -MSE: 13.2659817187 -train accuracy: 0.555878\n",
      "epoch: 423 - cost: 0.905018 -MSE: 14.5962280387 -train accuracy: 0.554064\n",
      "epoch: 424 - cost: 0.907807 -MSE: 13.161805994 -train accuracy: 0.558237\n",
      "epoch: 425 - cost: 0.901305 -MSE: 14.4787670878 -train accuracy: 0.554608\n",
      "epoch: 426 - cost: 0.904033 -MSE: 13.0597717132 -train accuracy: 0.558962\n",
      "epoch: 427 - cost: 0.897659 -MSE: 14.3634376886 -train accuracy: 0.556422\n",
      "epoch: 428 - cost: 0.900337 -MSE: 12.9605390473 -train accuracy: 0.559869\n",
      "epoch: 429 - cost: 0.894048 -MSE: 14.251807669 -train accuracy: 0.557329\n",
      "epoch: 430 - cost: 0.896639 -MSE: 12.863517375 -train accuracy: 0.560776\n",
      "epoch: 431 - cost: 0.890463 -MSE: 14.1420319734 -train accuracy: 0.558055\n",
      "epoch: 432 - cost: 0.892957 -MSE: 12.7672105808 -train accuracy: 0.562228\n",
      "epoch: 433 - cost: 0.886916 -MSE: 14.0330638875 -train accuracy: 0.558418\n",
      "epoch: 434 - cost: 0.889334 -MSE: 12.6718596682 -train accuracy: 0.562954\n",
      "epoch: 435 - cost: 0.883401 -MSE: 13.9252995378 -train accuracy: 0.559325\n",
      "epoch: 436 - cost: 0.88574 -MSE: 12.5777226117 -train accuracy: 0.563316\n",
      "epoch: 437 - cost: 0.879971 -MSE: 13.8180857111 -train accuracy: 0.559507\n",
      "epoch: 438 - cost: 0.882231 -MSE: 12.4841465018 -train accuracy: 0.563135\n",
      "epoch: 439 - cost: 0.87653 -MSE: 13.712732226 -train accuracy: 0.561321\n",
      "epoch: 440 - cost: 0.878685 -MSE: 12.3906231947 -train accuracy: 0.563861\n",
      "epoch: 441 - cost: 0.873116 -MSE: 13.6065089984 -train accuracy: 0.561684\n",
      "epoch: 442 - cost: 0.875255 -MSE: 12.2984455245 -train accuracy: 0.564586\n",
      "epoch: 443 - cost: 0.869708 -MSE: 13.5039969692 -train accuracy: 0.562409\n",
      "epoch: 444 - cost: 0.871812 -MSE: 12.2093812669 -train accuracy: 0.565493\n",
      "epoch: 445 - cost: 0.866385 -MSE: 13.4030280145 -train accuracy: 0.562591\n",
      "epoch: 446 - cost: 0.868447 -MSE: 12.1215499879 -train accuracy: 0.565493\n",
      "epoch: 447 - cost: 0.86311 -MSE: 13.3033463997 -train accuracy: 0.562772\n",
      "epoch: 448 - cost: 0.865112 -MSE: 12.0341968729 -train accuracy: 0.566038\n",
      "epoch: 449 - cost: 0.859902 -MSE: 13.2038770349 -train accuracy: 0.563498\n",
      "epoch: 450 - cost: 0.861842 -MSE: 11.9483764464 -train accuracy: 0.567308\n",
      "epoch: 451 - cost: 0.856719 -MSE: 13.1069685441 -train accuracy: 0.564586\n",
      "epoch: 452 - cost: 0.858608 -MSE: 11.8644599964 -train accuracy: 0.568396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 453 - cost: 0.853562 -MSE: 13.0121873092 -train accuracy: 0.565493\n",
      "epoch: 454 - cost: 0.855395 -MSE: 11.7816116648 -train accuracy: 0.569122\n",
      "epoch: 455 - cost: 0.850447 -MSE: 12.9179632595 -train accuracy: 0.567126\n",
      "epoch: 456 - cost: 0.852235 -MSE: 11.6994939931 -train accuracy: 0.570392\n",
      "epoch: 457 - cost: 0.847328 -MSE: 12.8262744509 -train accuracy: 0.567852\n",
      "epoch: 458 - cost: 0.84906 -MSE: 11.6193192081 -train accuracy: 0.572025\n",
      "epoch: 459 - cost: 0.844246 -MSE: 12.7350729315 -train accuracy: 0.568941\n",
      "epoch: 460 - cost: 0.845937 -MSE: 11.540094258 -train accuracy: 0.571843\n",
      "epoch: 461 - cost: 0.841236 -MSE: 12.6449582298 -train accuracy: 0.569848\n",
      "epoch: 462 - cost: 0.842816 -MSE: 11.461113372 -train accuracy: 0.572388\n",
      "epoch: 463 - cost: 0.838202 -MSE: 12.5560034727 -train accuracy: 0.571299\n",
      "epoch: 464 - cost: 0.839774 -MSE: 11.3846495459 -train accuracy: 0.573113\n",
      "epoch: 465 - cost: 0.835273 -MSE: 12.468438959 -train accuracy: 0.571843\n",
      "epoch: 466 - cost: 0.836781 -MSE: 11.3080768251 -train accuracy: 0.573476\n",
      "epoch: 467 - cost: 0.832359 -MSE: 12.3810913541 -train accuracy: 0.572569\n",
      "epoch: 468 - cost: 0.83377 -MSE: 11.2298927431 -train accuracy: 0.57402\n",
      "epoch: 469 - cost: 0.829448 -MSE: 12.2926436898 -train accuracy: 0.572388\n",
      "epoch: 470 - cost: 0.830819 -MSE: 11.1525151741 -train accuracy: 0.574383\n",
      "epoch: 471 - cost: 0.826596 -MSE: 12.2057915943 -train accuracy: 0.573476\n",
      "epoch: 472 - cost: 0.827941 -MSE: 11.0769005407 -train accuracy: 0.574746\n",
      "epoch: 473 - cost: 0.823823 -MSE: 12.1189043541 -train accuracy: 0.574565\n",
      "epoch: 474 - cost: 0.825135 -MSE: 11.0010132084 -train accuracy: 0.574927\n",
      "epoch: 475 - cost: 0.82106 -MSE: 12.0333608294 -train accuracy: 0.575472\n",
      "epoch: 476 - cost: 0.822308 -MSE: 10.9268804859 -train accuracy: 0.575653\n",
      "epoch: 477 - cost: 0.818271 -MSE: 11.9508645653 -train accuracy: 0.577467\n",
      "epoch: 478 - cost: 0.81947 -MSE: 10.8545524927 -train accuracy: 0.57656\n",
      "epoch: 479 - cost: 0.815507 -MSE: 11.8686234795 -train accuracy: 0.578012\n",
      "epoch: 480 - cost: 0.816644 -MSE: 10.7824208074 -train accuracy: 0.577467\n",
      "epoch: 481 - cost: 0.812773 -MSE: 11.786999542 -train accuracy: 0.579463\n",
      "epoch: 482 - cost: 0.813878 -MSE: 10.710863732 -train accuracy: 0.578193\n",
      "epoch: 483 - cost: 0.810076 -MSE: 11.7067030311 -train accuracy: 0.580189\n",
      "epoch: 484 - cost: 0.811129 -MSE: 10.6398933666 -train accuracy: 0.579282\n",
      "epoch: 485 - cost: 0.807408 -MSE: 11.6267462381 -train accuracy: 0.580914\n",
      "epoch: 486 - cost: 0.808393 -MSE: 10.5692566681 -train accuracy: 0.579826\n",
      "epoch: 487 - cost: 0.804771 -MSE: 11.5473390321 -train accuracy: 0.581822\n",
      "epoch: 488 - cost: 0.805694 -MSE: 10.499086249 -train accuracy: 0.580552\n",
      "epoch: 489 - cost: 0.802187 -MSE: 11.4675181011 -train accuracy: 0.582366\n",
      "epoch: 490 - cost: 0.803068 -MSE: 10.4293601308 -train accuracy: 0.582547\n",
      "epoch: 491 - cost: 0.799614 -MSE: 11.3885138626 -train accuracy: 0.582003\n",
      "epoch: 492 - cost: 0.800472 -MSE: 10.3601501182 -train accuracy: 0.58291\n",
      "epoch: 493 - cost: 0.797075 -MSE: 11.3105034834 -train accuracy: 0.582729\n",
      "epoch: 494 - cost: 0.797897 -MSE: 10.291519785 -train accuracy: 0.583999\n",
      "epoch: 495 - cost: 0.794587 -MSE: 11.2334445391 -train accuracy: 0.583454\n",
      "epoch: 496 - cost: 0.795341 -MSE: 10.2233948785 -train accuracy: 0.584906\n",
      "epoch: 497 - cost: 0.792091 -MSE: 11.1569851379 -train accuracy: 0.584906\n",
      "epoch: 498 - cost: 0.792787 -MSE: 10.1554854288 -train accuracy: 0.585994\n",
      "epoch: 499 - cost: 0.789595 -MSE: 11.0806881246 -train accuracy: 0.585268\n",
      "epoch: 500 - cost: 0.790223 -MSE: 10.0876421636 -train accuracy: 0.58672\n",
      "epoch: 501 - cost: 0.787068 -MSE: 11.0049269605 -train accuracy: 0.587264\n",
      "epoch: 502 - cost: 0.787675 -MSE: 10.0209236588 -train accuracy: 0.588171\n",
      "epoch: 503 - cost: 0.78461 -MSE: 10.9302469618 -train accuracy: 0.587808\n",
      "epoch: 504 - cost: 0.785133 -MSE: 9.95367939982 -train accuracy: 0.589985\n",
      "epoch: 505 - cost: 0.78216 -MSE: 10.8540999528 -train accuracy: 0.588534\n",
      "epoch: 506 - cost: 0.782668 -MSE: 9.88643799519 -train accuracy: 0.590893\n",
      "epoch: 507 - cost: 0.779754 -MSE: 10.7791635683 -train accuracy: 0.58799\n",
      "epoch: 508 - cost: 0.780155 -MSE: 9.81854354326 -train accuracy: 0.592163\n",
      "epoch: 509 - cost: 0.777272 -MSE: 10.7028452793 -train accuracy: 0.589441\n",
      "epoch: 510 - cost: 0.777657 -MSE: 9.75122378185 -train accuracy: 0.593795\n",
      "epoch: 511 - cost: 0.774835 -MSE: 10.6274230465 -train accuracy: 0.591618\n",
      "epoch: 512 - cost: 0.775168 -MSE: 9.68336512925 -train accuracy: 0.595065\n",
      "epoch: 513 - cost: 0.772375 -MSE: 10.5524846431 -train accuracy: 0.591981\n",
      "epoch: 514 - cost: 0.772643 -MSE: 9.61556364618 -train accuracy: 0.595791\n",
      "epoch: 515 - cost: 0.769885 -MSE: 10.4768238596 -train accuracy: 0.593432\n",
      "epoch: 516 - cost: 0.770151 -MSE: 9.5488681616 -train accuracy: 0.596517\n",
      "epoch: 517 - cost: 0.76739 -MSE: 10.4034968172 -train accuracy: 0.594158\n",
      "epoch: 518 - cost: 0.767583 -MSE: 9.48273599279 -train accuracy: 0.597242\n",
      "epoch: 519 - cost: 0.764913 -MSE: 10.328822294 -train accuracy: 0.595247\n",
      "epoch: 520 - cost: 0.765089 -MSE: 9.41690400234 -train accuracy: 0.598694\n",
      "epoch: 521 - cost: 0.762455 -MSE: 10.2558280829 -train accuracy: 0.596335\n",
      "epoch: 522 - cost: 0.762514 -MSE: 9.35030038978 -train accuracy: 0.599782\n",
      "epoch: 523 - cost: 0.759976 -MSE: 10.1810005476 -train accuracy: 0.598331\n",
      "epoch: 524 - cost: 0.759975 -MSE: 9.28263108084 -train accuracy: 0.600327\n",
      "epoch: 525 - cost: 0.757492 -MSE: 10.1059556829 -train accuracy: 0.600145\n",
      "epoch: 526 - cost: 0.757408 -MSE: 9.21393138768 -train accuracy: 0.601052\n",
      "epoch: 527 - cost: 0.754946 -MSE: 10.0306018734 -train accuracy: 0.600689\n",
      "epoch: 528 - cost: 0.754714 -MSE: 9.14456204697 -train accuracy: 0.601778\n",
      "epoch: 529 - cost: 0.75231 -MSE: 9.95434289771 -train accuracy: 0.600508\n",
      "epoch: 530 - cost: 0.751947 -MSE: 9.07391985689 -train accuracy: 0.602866\n",
      "epoch: 531 - cost: 0.74958 -MSE: 9.87672229326 -train accuracy: 0.601959\n",
      "epoch: 532 - cost: 0.749147 -MSE: 9.0035065813 -train accuracy: 0.603592\n",
      "epoch: 533 - cost: 0.746768 -MSE: 9.80048536528 -train accuracy: 0.603955\n",
      "epoch: 534 - cost: 0.746262 -MSE: 8.93379127476 -train accuracy: 0.604862\n",
      "epoch: 535 - cost: 0.743908 -MSE: 9.72372494306 -train accuracy: 0.606495\n",
      "epoch: 536 - cost: 0.743263 -MSE: 8.86236856935 -train accuracy: 0.605951\n",
      "epoch: 537 - cost: 0.740849 -MSE: 9.64605521164 -train accuracy: 0.607946\n",
      "epoch: 538 - cost: 0.740112 -MSE: 8.79097419624 -train accuracy: 0.607039\n",
      "epoch: 539 - cost: 0.737697 -MSE: 9.5679069018 -train accuracy: 0.609579\n",
      "epoch: 540 - cost: 0.736835 -MSE: 8.71828712347 -train accuracy: 0.608672\n",
      "epoch: 541 - cost: 0.734376 -MSE: 9.48949174715 -train accuracy: 0.610123\n",
      "epoch: 542 - cost: 0.733488 -MSE: 8.64697883376 -train accuracy: 0.610486\n",
      "epoch: 543 - cost: 0.730936 -MSE: 9.41367560807 -train accuracy: 0.610305\n",
      "epoch: 544 - cost: 0.72997 -MSE: 8.57731720669 -train accuracy: 0.6123\n",
      "epoch: 545 - cost: 0.727468 -MSE: 9.33843688698 -train accuracy: 0.610668\n",
      "epoch: 546 - cost: 0.726504 -MSE: 8.51031971197 -train accuracy: 0.61357\n",
      "epoch: 547 - cost: 0.72398 -MSE: 9.26732992409 -train accuracy: 0.611938\n",
      "epoch: 548 - cost: 0.722945 -MSE: 8.44556917023 -train accuracy: 0.615929\n",
      "epoch: 549 - cost: 0.720413 -MSE: 9.19799262678 -train accuracy: 0.61357\n",
      "epoch: 550 - cost: 0.719355 -MSE: 8.38299529201 -train accuracy: 0.616836\n",
      "epoch: 551 - cost: 0.716904 -MSE: 9.12937512655 -train accuracy: 0.613933\n",
      "epoch: 552 - cost: 0.715741 -MSE: 8.31987157065 -train accuracy: 0.617562\n",
      "epoch: 553 - cost: 0.713316 -MSE: 9.06035023027 -train accuracy: 0.615929\n",
      "epoch: 554 - cost: 0.712179 -MSE: 8.25917332351 -train accuracy: 0.619376\n",
      "epoch: 555 - cost: 0.709643 -MSE: 8.99727985432 -train accuracy: 0.61738\n",
      "epoch: 556 - cost: 0.708484 -MSE: 8.20129793548 -train accuracy: 0.620646\n",
      "epoch: 557 - cost: 0.706004 -MSE: 8.93381276863 -train accuracy: 0.619739\n",
      "epoch: 558 - cost: 0.704996 -MSE: 8.14705372454 -train accuracy: 0.62119\n",
      "epoch: 559 - cost: 0.702531 -MSE: 8.87558520616 -train accuracy: 0.622642\n",
      "epoch: 560 - cost: 0.701692 -MSE: 8.09758044957 -train accuracy: 0.623367\n",
      "epoch: 561 - cost: 0.699281 -MSE: 8.82175802161 -train accuracy: 0.624456\n",
      "epoch: 562 - cost: 0.698624 -MSE: 8.0534956697 -train accuracy: 0.625363\n",
      "epoch: 563 - cost: 0.696373 -MSE: 8.77120961175 -train accuracy: 0.626451\n",
      "epoch: 564 - cost: 0.695891 -MSE: 8.01289442258 -train accuracy: 0.626089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 565 - cost: 0.693789 -MSE: 8.72354047333 -train accuracy: 0.629717\n",
      "epoch: 566 - cost: 0.693406 -MSE: 7.97300836507 -train accuracy: 0.627721\n",
      "epoch: 567 - cost: 0.691397 -MSE: 8.67837397948 -train accuracy: 0.631168\n",
      "epoch: 568 - cost: 0.691078 -MSE: 7.93513435279 -train accuracy: 0.62881\n",
      "epoch: 569 - cost: 0.689135 -MSE: 8.6362203189 -train accuracy: 0.633345\n",
      "epoch: 570 - cost: 0.688858 -MSE: 7.89896790417 -train accuracy: 0.629898\n",
      "epoch: 571 - cost: 0.686986 -MSE: 8.5953393853 -train accuracy: 0.63516\n",
      "epoch: 572 - cost: 0.686809 -MSE: 7.86637673789 -train accuracy: 0.630443\n",
      "epoch: 573 - cost: 0.684996 -MSE: 8.55821145415 -train accuracy: 0.635704\n",
      "epoch: 574 - cost: 0.684927 -MSE: 7.83732396057 -train accuracy: 0.630805\n",
      "epoch: 575 - cost: 0.683221 -MSE: 8.52374557441 -train accuracy: 0.636248\n",
      "epoch: 576 - cost: 0.683253 -MSE: 7.81024014745 -train accuracy: 0.632257\n",
      "epoch: 577 - cost: 0.681658 -MSE: 8.49092792083 -train accuracy: 0.636792\n",
      "epoch: 578 - cost: 0.681738 -MSE: 7.78338072715 -train accuracy: 0.632801\n",
      "epoch: 579 - cost: 0.680172 -MSE: 8.45951184143 -train accuracy: 0.638607\n",
      "epoch: 580 - cost: 0.680326 -MSE: 7.75890171308 -train accuracy: 0.632983\n",
      "epoch: 581 - cost: 0.678821 -MSE: 8.43034295515 -train accuracy: 0.639151\n",
      "epoch: 582 - cost: 0.678997 -MSE: 7.73514488686 -train accuracy: 0.633527\n",
      "epoch: 583 - cost: 0.677561 -MSE: 8.4014285808 -train accuracy: 0.63897\n",
      "epoch: 584 - cost: 0.677785 -MSE: 7.71225325047 -train accuracy: 0.634434\n",
      "epoch: 585 - cost: 0.676397 -MSE: 8.37400748447 -train accuracy: 0.639151\n",
      "epoch: 586 - cost: 0.676608 -MSE: 7.6890155017 -train accuracy: 0.635522\n",
      "epoch: 587 - cost: 0.675232 -MSE: 8.34625469143 -train accuracy: 0.640239\n",
      "epoch: 588 - cost: 0.675474 -MSE: 7.6671262661 -train accuracy: 0.636067\n",
      "epoch: 589 - cost: 0.674157 -MSE: 8.31947771032 -train accuracy: 0.640784\n",
      "epoch: 590 - cost: 0.674397 -MSE: 7.64453684456 -train accuracy: 0.63643\n",
      "epoch: 591 - cost: 0.67312 -MSE: 8.29240310061 -train accuracy: 0.641147\n",
      "epoch: 592 - cost: 0.673331 -MSE: 7.62090876332 -train accuracy: 0.636248\n",
      "epoch: 593 - cost: 0.672068 -MSE: 8.26539579707 -train accuracy: 0.642054\n",
      "epoch: 594 - cost: 0.672291 -MSE: 7.59822078013 -train accuracy: 0.636792\n",
      "epoch: 595 - cost: 0.671036 -MSE: 8.23886009441 -train accuracy: 0.642779\n",
      "epoch: 596 - cost: 0.671275 -MSE: 7.57664073143 -train accuracy: 0.636611\n",
      "epoch: 597 - cost: 0.670037 -MSE: 8.21338520705 -train accuracy: 0.643142\n",
      "epoch: 598 - cost: 0.670259 -MSE: 7.55453427076 -train accuracy: 0.637518\n",
      "epoch: 599 - cost: 0.66903 -MSE: 8.18752767556 -train accuracy: 0.643324\n",
      "epoch: 600 - cost: 0.669278 -MSE: 7.5335338693 -train accuracy: 0.637155\n",
      "epoch: 601 - cost: 0.668063 -MSE: 8.16280618645 -train accuracy: 0.643324\n",
      "epoch: 602 - cost: 0.668311 -MSE: 7.51270596419 -train accuracy: 0.637518\n",
      "epoch: 603 - cost: 0.667126 -MSE: 8.13738654198 -train accuracy: 0.643505\n",
      "epoch: 604 - cost: 0.667397 -MSE: 7.49247838151 -train accuracy: 0.6377\n",
      "epoch: 605 - cost: 0.666221 -MSE: 8.11356738744 -train accuracy: 0.643868\n",
      "epoch: 606 - cost: 0.666502 -MSE: 7.47227816089 -train accuracy: 0.637881\n",
      "epoch: 607 - cost: 0.665349 -MSE: 8.08905492743 -train accuracy: 0.643686\n",
      "epoch: 608 - cost: 0.66563 -MSE: 7.45188727374 -train accuracy: 0.638244\n",
      "epoch: 609 - cost: 0.664483 -MSE: 8.06487852578 -train accuracy: 0.643868\n",
      "epoch: 610 - cost: 0.664747 -MSE: 7.43127725201 -train accuracy: 0.638244\n",
      "epoch: 611 - cost: 0.663609 -MSE: 8.04063389053 -train accuracy: 0.644412\n",
      "epoch: 612 - cost: 0.663873 -MSE: 7.41095914394 -train accuracy: 0.639151\n",
      "epoch: 613 - cost: 0.662754 -MSE: 8.01650170769 -train accuracy: 0.644412\n",
      "epoch: 614 - cost: 0.663011 -MSE: 7.39063994129 -train accuracy: 0.639332\n",
      "epoch: 615 - cost: 0.6619 -MSE: 7.99242124555 -train accuracy: 0.645138\n",
      "epoch: 616 - cost: 0.662161 -MSE: 7.37129815997 -train accuracy: 0.639695\n",
      "epoch: 617 - cost: 0.661079 -MSE: 7.96906224267 -train accuracy: 0.645682\n",
      "epoch: 618 - cost: 0.661328 -MSE: 7.35183403107 -train accuracy: 0.639332\n",
      "epoch: 619 - cost: 0.660245 -MSE: 7.94686140693 -train accuracy: 0.645864\n",
      "epoch: 620 - cost: 0.660503 -MSE: 7.33395427586 -train accuracy: 0.639514\n",
      "epoch: 621 - cost: 0.659431 -MSE: 7.92515431033 -train accuracy: 0.646045\n",
      "epoch: 622 - cost: 0.659711 -MSE: 7.3169822993 -train accuracy: 0.640058\n",
      "epoch: 623 - cost: 0.658647 -MSE: 7.90493194799 -train accuracy: 0.646589\n",
      "epoch: 624 - cost: 0.658922 -MSE: 7.30052188052 -train accuracy: 0.640965\n",
      "epoch: 625 - cost: 0.657872 -MSE: 7.88487574808 -train accuracy: 0.647678\n",
      "epoch: 626 - cost: 0.658128 -MSE: 7.28348325615 -train accuracy: 0.641328\n",
      "epoch: 627 - cost: 0.657077 -MSE: 7.86477015048 -train accuracy: 0.648041\n",
      "epoch: 628 - cost: 0.657313 -MSE: 7.26600390574 -train accuracy: 0.642235\n",
      "epoch: 629 - cost: 0.656251 -MSE: 7.84489674858 -train accuracy: 0.647496\n",
      "epoch: 630 - cost: 0.65651 -MSE: 7.25097763406 -train accuracy: 0.642598\n",
      "epoch: 631 - cost: 0.655455 -MSE: 7.82682957304 -train accuracy: 0.648222\n",
      "epoch: 632 - cost: 0.655705 -MSE: 7.23626809576 -train accuracy: 0.642961\n",
      "epoch: 633 - cost: 0.654664 -MSE: 7.80853568608 -train accuracy: 0.648766\n",
      "epoch: 634 - cost: 0.654914 -MSE: 7.22183978715 -train accuracy: 0.643324\n",
      "epoch: 635 - cost: 0.653886 -MSE: 7.79072861035 -train accuracy: 0.649492\n",
      "epoch: 636 - cost: 0.654141 -MSE: 7.2073607386 -train accuracy: 0.643686\n",
      "epoch: 637 - cost: 0.653124 -MSE: 7.77310502344 -train accuracy: 0.650581\n",
      "epoch: 638 - cost: 0.65337 -MSE: 7.19225409323 -train accuracy: 0.644049\n",
      "epoch: 639 - cost: 0.652372 -MSE: 7.75456112843 -train accuracy: 0.651488\n",
      "epoch: 640 - cost: 0.65262 -MSE: 7.17743866335 -train accuracy: 0.644594\n",
      "epoch: 641 - cost: 0.651625 -MSE: 7.73690079252 -train accuracy: 0.651488\n",
      "epoch: 642 - cost: 0.651873 -MSE: 7.16329164238 -train accuracy: 0.645138\n",
      "epoch: 643 - cost: 0.650891 -MSE: 7.71942196364 -train accuracy: 0.651669\n",
      "epoch: 644 - cost: 0.651119 -MSE: 7.14827182554 -train accuracy: 0.646045\n",
      "epoch: 645 - cost: 0.650153 -MSE: 7.70127951256 -train accuracy: 0.651488\n",
      "epoch: 646 - cost: 0.650388 -MSE: 7.13401669876 -train accuracy: 0.646226\n",
      "epoch: 647 - cost: 0.649423 -MSE: 7.68487769673 -train accuracy: 0.652395\n",
      "epoch: 648 - cost: 0.649645 -MSE: 7.12042027552 -train accuracy: 0.646226\n",
      "epoch: 649 - cost: 0.648667 -MSE: 7.66896580198 -train accuracy: 0.652758\n",
      "epoch: 650 - cost: 0.648883 -MSE: 7.10748527991 -train accuracy: 0.646226\n",
      "epoch: 651 - cost: 0.647931 -MSE: 7.65193725963 -train accuracy: 0.653483\n",
      "epoch: 652 - cost: 0.648149 -MSE: 7.09437027697 -train accuracy: 0.646408\n",
      "epoch: 653 - cost: 0.647211 -MSE: 7.63562462792 -train accuracy: 0.653483\n",
      "epoch: 654 - cost: 0.647424 -MSE: 7.08124784849 -train accuracy: 0.646952\n",
      "epoch: 655 - cost: 0.646498 -MSE: 7.61948634074 -train accuracy: 0.654935\n",
      "epoch: 656 - cost: 0.646697 -MSE: 7.06779714836 -train accuracy: 0.647496\n",
      "epoch: 657 - cost: 0.645786 -MSE: 7.6028868412 -train accuracy: 0.655842\n",
      "epoch: 658 - cost: 0.645971 -MSE: 7.05396481152 -train accuracy: 0.648222\n",
      "epoch: 659 - cost: 0.645074 -MSE: 7.58665591609 -train accuracy: 0.656568\n",
      "epoch: 660 - cost: 0.645248 -MSE: 7.04041632118 -train accuracy: 0.648403\n",
      "epoch: 661 - cost: 0.64436 -MSE: 7.57065419851 -train accuracy: 0.657293\n",
      "epoch: 662 - cost: 0.644539 -MSE: 7.02793349995 -train accuracy: 0.649129\n",
      "epoch: 663 - cost: 0.643663 -MSE: 7.55563909768 -train accuracy: 0.657656\n",
      "epoch: 664 - cost: 0.643842 -MSE: 7.01603730121 -train accuracy: 0.649855\n",
      "epoch: 665 - cost: 0.642997 -MSE: 7.5406234386 -train accuracy: 0.6582\n",
      "epoch: 666 - cost: 0.643182 -MSE: 7.0046485502 -train accuracy: 0.650581\n",
      "epoch: 667 - cost: 0.642352 -MSE: 7.52641597046 -train accuracy: 0.658563\n",
      "epoch: 668 - cost: 0.64253 -MSE: 6.99279843343 -train accuracy: 0.651125\n",
      "epoch: 669 - cost: 0.641722 -MSE: 7.51142458476 -train accuracy: 0.658382\n",
      "epoch: 670 - cost: 0.641892 -MSE: 6.98040342983 -train accuracy: 0.651851\n",
      "epoch: 671 - cost: 0.641105 -MSE: 7.49592249054 -train accuracy: 0.659107\n",
      "epoch: 672 - cost: 0.64128 -MSE: 6.96801118445 -train accuracy: 0.652395\n",
      "epoch: 673 - cost: 0.640505 -MSE: 7.48118027209 -train accuracy: 0.659652\n",
      "epoch: 674 - cost: 0.64067 -MSE: 6.95565178666 -train accuracy: 0.652758\n",
      "epoch: 675 - cost: 0.63991 -MSE: 7.46603211318 -train accuracy: 0.65947\n",
      "epoch: 676 - cost: 0.640083 -MSE: 6.94388848006 -train accuracy: 0.652939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 677 - cost: 0.639329 -MSE: 7.45190796607 -train accuracy: 0.659652\n",
      "epoch: 678 - cost: 0.639508 -MSE: 6.93295387331 -train accuracy: 0.653846\n",
      "epoch: 679 - cost: 0.63875 -MSE: 7.43879811266 -train accuracy: 0.661103\n",
      "epoch: 680 - cost: 0.63893 -MSE: 6.92272498231 -train accuracy: 0.654209\n",
      "epoch: 681 - cost: 0.638175 -MSE: 7.4264957104 -train accuracy: 0.662373\n",
      "epoch: 682 - cost: 0.638346 -MSE: 6.91272984632 -train accuracy: 0.654028\n",
      "epoch: 683 - cost: 0.637597 -MSE: 7.41419938108 -train accuracy: 0.663099\n",
      "epoch: 684 - cost: 0.63776 -MSE: 6.9027962194 -train accuracy: 0.653846\n",
      "epoch: 685 - cost: 0.63703 -MSE: 7.40178462326 -train accuracy: 0.663462\n",
      "epoch: 686 - cost: 0.637191 -MSE: 6.89270515758 -train accuracy: 0.654028\n",
      "epoch: 687 - cost: 0.636483 -MSE: 7.38898181714 -train accuracy: 0.664187\n",
      "epoch: 688 - cost: 0.636637 -MSE: 6.88199784005 -train accuracy: 0.654028\n",
      "epoch: 689 - cost: 0.635935 -MSE: 7.37605125194 -train accuracy: 0.66455\n",
      "epoch: 690 - cost: 0.636087 -MSE: 6.87167492491 -train accuracy: 0.65439\n",
      "epoch: 691 - cost: 0.635393 -MSE: 7.36329299628 -train accuracy: 0.66455\n",
      "epoch: 692 - cost: 0.635534 -MSE: 6.86097356556 -train accuracy: 0.654753\n",
      "epoch: 693 - cost: 0.634844 -MSE: 7.3503275183 -train accuracy: 0.665094\n",
      "epoch: 694 - cost: 0.634987 -MSE: 6.8508565495 -train accuracy: 0.654572\n",
      "epoch: 695 - cost: 0.634294 -MSE: 7.33845261436 -train accuracy: 0.665276\n",
      "epoch: 696 - cost: 0.63444 -MSE: 6.84183328847 -train accuracy: 0.655298\n",
      "epoch: 697 - cost: 0.633753 -MSE: 7.32734679705 -train accuracy: 0.665639\n",
      "epoch: 698 - cost: 0.633894 -MSE: 6.83273247037 -train accuracy: 0.655116\n",
      "epoch: 699 - cost: 0.633216 -MSE: 7.31592403189 -train accuracy: 0.66582\n",
      "epoch: 700 - cost: 0.633354 -MSE: 6.82379849781 -train accuracy: 0.654572\n",
      "epoch: 701 - cost: 0.632688 -MSE: 7.30473598419 -train accuracy: 0.66582\n",
      "epoch: 702 - cost: 0.632819 -MSE: 6.81494931391 -train accuracy: 0.655116\n",
      "epoch: 703 - cost: 0.632163 -MSE: 7.29357265325 -train accuracy: 0.666001\n",
      "epoch: 704 - cost: 0.632299 -MSE: 6.80658141714 -train accuracy: 0.655479\n",
      "epoch: 705 - cost: 0.631649 -MSE: 7.28303537746 -train accuracy: 0.666183\n",
      "epoch: 706 - cost: 0.631782 -MSE: 6.79843922584 -train accuracy: 0.655842\n",
      "epoch: 707 - cost: 0.631155 -MSE: 7.27244751651 -train accuracy: 0.666364\n",
      "epoch: 708 - cost: 0.631296 -MSE: 6.7908014966 -train accuracy: 0.656023\n",
      "epoch: 709 - cost: 0.630673 -MSE: 7.26303074968 -train accuracy: 0.666364\n",
      "epoch: 710 - cost: 0.630793 -MSE: 6.78307046707 -train accuracy: 0.655842\n",
      "epoch: 711 - cost: 0.630178 -MSE: 7.25315611287 -train accuracy: 0.666727\n",
      "epoch: 712 - cost: 0.630286 -MSE: 6.77509566826 -train accuracy: 0.656205\n",
      "epoch: 713 - cost: 0.629683 -MSE: 7.24282159833 -train accuracy: 0.667997\n",
      "epoch: 714 - cost: 0.62979 -MSE: 6.76710947612 -train accuracy: 0.657656\n",
      "epoch: 715 - cost: 0.62919 -MSE: 7.23280716895 -train accuracy: 0.667997\n",
      "epoch: 716 - cost: 0.629298 -MSE: 6.75957068164 -train accuracy: 0.658019\n",
      "epoch: 717 - cost: 0.6287 -MSE: 7.22334257885 -train accuracy: 0.668541\n",
      "epoch: 718 - cost: 0.628811 -MSE: 6.75282847116 -train accuracy: 0.658382\n",
      "epoch: 719 - cost: 0.628223 -MSE: 7.21430097863 -train accuracy: 0.668541\n",
      "epoch: 720 - cost: 0.628328 -MSE: 6.74567044747 -train accuracy: 0.658745\n",
      "epoch: 721 - cost: 0.627752 -MSE: 7.20490207683 -train accuracy: 0.668904\n",
      "epoch: 722 - cost: 0.627855 -MSE: 6.73863976127 -train accuracy: 0.658926\n",
      "epoch: 723 - cost: 0.627285 -MSE: 7.19590315053 -train accuracy: 0.669267\n",
      "epoch: 724 - cost: 0.627373 -MSE: 6.73143927866 -train accuracy: 0.659289\n",
      "epoch: 725 - cost: 0.626809 -MSE: 7.18679798916 -train accuracy: 0.669448\n",
      "epoch: 726 - cost: 0.626889 -MSE: 6.72409032936 -train accuracy: 0.65947\n",
      "epoch: 727 - cost: 0.626327 -MSE: 7.17782303908 -train accuracy: 0.670174\n",
      "epoch: 728 - cost: 0.626402 -MSE: 6.71712051462 -train accuracy: 0.659833\n",
      "epoch: 729 - cost: 0.625854 -MSE: 7.16866535261 -train accuracy: 0.670356\n",
      "epoch: 730 - cost: 0.625932 -MSE: 6.71059478036 -train accuracy: 0.65947\n",
      "epoch: 731 - cost: 0.625388 -MSE: 7.16029317446 -train accuracy: 0.670174\n",
      "epoch: 732 - cost: 0.625461 -MSE: 6.70465508926 -train accuracy: 0.65947\n",
      "epoch: 733 - cost: 0.624921 -MSE: 7.15265469253 -train accuracy: 0.670356\n",
      "epoch: 734 - cost: 0.624985 -MSE: 6.69896875175 -train accuracy: 0.65947\n",
      "epoch: 735 - cost: 0.624457 -MSE: 7.14482204248 -train accuracy: 0.670718\n",
      "epoch: 736 - cost: 0.624522 -MSE: 6.69345850775 -train accuracy: 0.66074\n",
      "epoch: 737 - cost: 0.623998 -MSE: 7.13766378492 -train accuracy: 0.671081\n",
      "epoch: 738 - cost: 0.624054 -MSE: 6.68819264111 -train accuracy: 0.660196\n",
      "epoch: 739 - cost: 0.623541 -MSE: 7.13010830402 -train accuracy: 0.67217\n",
      "epoch: 740 - cost: 0.623605 -MSE: 6.68333228905 -train accuracy: 0.660559\n",
      "epoch: 741 - cost: 0.623105 -MSE: 7.12293884161 -train accuracy: 0.672351\n",
      "epoch: 742 - cost: 0.623162 -MSE: 6.67787410006 -train accuracy: 0.661103\n",
      "epoch: 743 - cost: 0.622663 -MSE: 7.11563864552 -train accuracy: 0.672351\n",
      "epoch: 744 - cost: 0.622712 -MSE: 6.67219864272 -train accuracy: 0.661103\n",
      "epoch: 745 - cost: 0.622219 -MSE: 7.10814316688 -train accuracy: 0.672714\n",
      "epoch: 746 - cost: 0.622264 -MSE: 6.6666843187 -train accuracy: 0.661647\n",
      "epoch: 747 - cost: 0.621788 -MSE: 7.10001913076 -train accuracy: 0.673621\n",
      "epoch: 748 - cost: 0.621831 -MSE: 6.66059256486 -train accuracy: 0.66201\n",
      "epoch: 749 - cost: 0.621364 -MSE: 7.09149894649 -train accuracy: 0.673984\n",
      "epoch: 750 - cost: 0.621406 -MSE: 6.65413321999 -train accuracy: 0.662373\n",
      "epoch: 751 - cost: 0.620941 -MSE: 7.08344332535 -train accuracy: 0.674528\n",
      "epoch: 752 - cost: 0.620977 -MSE: 6.64766735125 -train accuracy: 0.663099\n",
      "epoch: 753 - cost: 0.620524 -MSE: 7.07461520685 -train accuracy: 0.675435\n",
      "epoch: 754 - cost: 0.620553 -MSE: 6.64061115712 -train accuracy: 0.663099\n",
      "epoch: 755 - cost: 0.620105 -MSE: 7.06602767871 -train accuracy: 0.67598\n",
      "epoch: 756 - cost: 0.620133 -MSE: 6.6338679409 -train accuracy: 0.663099\n",
      "epoch: 757 - cost: 0.619687 -MSE: 7.057407758 -train accuracy: 0.676705\n",
      "epoch: 758 - cost: 0.619714 -MSE: 6.62724628307 -train accuracy: 0.663462\n",
      "epoch: 759 - cost: 0.619273 -MSE: 7.04890361453 -train accuracy: 0.677431\n",
      "epoch: 760 - cost: 0.619302 -MSE: 6.62080638716 -train accuracy: 0.663643\n",
      "epoch: 761 - cost: 0.618862 -MSE: 7.04089642425 -train accuracy: 0.677794\n",
      "epoch: 762 - cost: 0.618881 -MSE: 6.61410509051 -train accuracy: 0.664187\n",
      "epoch: 763 - cost: 0.618446 -MSE: 7.03279064143 -train accuracy: 0.677612\n",
      "epoch: 764 - cost: 0.618464 -MSE: 6.60811022399 -train accuracy: 0.664732\n",
      "epoch: 765 - cost: 0.618023 -MSE: 7.02539302876 -train accuracy: 0.67852\n",
      "epoch: 766 - cost: 0.618039 -MSE: 6.60253793193 -train accuracy: 0.665094\n",
      "epoch: 767 - cost: 0.6176 -MSE: 7.01824232357 -train accuracy: 0.678338\n",
      "epoch: 768 - cost: 0.617618 -MSE: 6.59753555336 -train accuracy: 0.664913\n",
      "epoch: 769 - cost: 0.617182 -MSE: 7.01139819411 -train accuracy: 0.67852\n",
      "epoch: 770 - cost: 0.617203 -MSE: 6.59279166618 -train accuracy: 0.664732\n",
      "epoch: 771 - cost: 0.616782 -MSE: 7.00447806323 -train accuracy: 0.679064\n",
      "epoch: 772 - cost: 0.616803 -MSE: 6.58745308218 -train accuracy: 0.664913\n",
      "epoch: 773 - cost: 0.616389 -MSE: 6.99719778182 -train accuracy: 0.679427\n",
      "epoch: 774 - cost: 0.616403 -MSE: 6.58173436678 -train accuracy: 0.664732\n",
      "epoch: 775 - cost: 0.616003 -MSE: 6.98915189177 -train accuracy: 0.679245\n",
      "epoch: 776 - cost: 0.616017 -MSE: 6.57550081454 -train accuracy: 0.664913\n",
      "epoch: 777 - cost: 0.61561 -MSE: 6.98182995352 -train accuracy: 0.679971\n",
      "epoch: 778 - cost: 0.615625 -MSE: 6.56983190076 -train accuracy: 0.664732\n",
      "epoch: 779 - cost: 0.615222 -MSE: 6.97436242089 -train accuracy: 0.680152\n",
      "epoch: 780 - cost: 0.61523 -MSE: 6.56377952227 -train accuracy: 0.664913\n",
      "epoch: 781 - cost: 0.614824 -MSE: 6.96700207771 -train accuracy: 0.680878\n",
      "epoch: 782 - cost: 0.614836 -MSE: 6.55837753905 -train accuracy: 0.664732\n",
      "epoch: 783 - cost: 0.614437 -MSE: 6.95999043497 -train accuracy: 0.681967\n",
      "epoch: 784 - cost: 0.614446 -MSE: 6.55292012732 -train accuracy: 0.665276\n",
      "epoch: 785 - cost: 0.61406 -MSE: 6.95237604249 -train accuracy: 0.681967\n",
      "epoch: 786 - cost: 0.614067 -MSE: 6.54699757545 -train accuracy: 0.665276\n",
      "epoch: 787 - cost: 0.613684 -MSE: 6.94492812051 -train accuracy: 0.681604\n",
      "epoch: 788 - cost: 0.613699 -MSE: 6.54162453366 -train accuracy: 0.664913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 789 - cost: 0.613328 -MSE: 6.93739217289 -train accuracy: 0.681785\n",
      "epoch: 790 - cost: 0.613345 -MSE: 6.53589897371 -train accuracy: 0.665276\n",
      "epoch: 791 - cost: 0.612971 -MSE: 6.93038383112 -train accuracy: 0.682511\n",
      "epoch: 792 - cost: 0.612987 -MSE: 6.5306954656 -train accuracy: 0.665276\n",
      "epoch: 793 - cost: 0.612613 -MSE: 6.923760137 -train accuracy: 0.683237\n",
      "epoch: 794 - cost: 0.612625 -MSE: 6.52557283586 -train accuracy: 0.665457\n",
      "epoch: 795 - cost: 0.612253 -MSE: 6.91715706448 -train accuracy: 0.683237\n",
      "epoch: 796 - cost: 0.612264 -MSE: 6.52050736836 -train accuracy: 0.66582\n",
      "epoch: 797 - cost: 0.611896 -MSE: 6.91043579025 -train accuracy: 0.683599\n",
      "epoch: 798 - cost: 0.611911 -MSE: 6.51575268789 -train accuracy: 0.66582\n",
      "epoch: 799 - cost: 0.611551 -MSE: 6.90375823015 -train accuracy: 0.684507\n",
      "epoch: 800 - cost: 0.611565 -MSE: 6.51063896707 -train accuracy: 0.66582\n",
      "epoch: 801 - cost: 0.611208 -MSE: 6.8971056218 -train accuracy: 0.685051\n",
      "epoch: 802 - cost: 0.611221 -MSE: 6.50548288906 -train accuracy: 0.666001\n",
      "epoch: 803 - cost: 0.610865 -MSE: 6.89047838663 -train accuracy: 0.685232\n",
      "epoch: 804 - cost: 0.610875 -MSE: 6.50007551904 -train accuracy: 0.666909\n",
      "epoch: 805 - cost: 0.610522 -MSE: 6.88356284285 -train accuracy: 0.685595\n",
      "epoch: 806 - cost: 0.610532 -MSE: 6.49471653481 -train accuracy: 0.66709\n",
      "epoch: 807 - cost: 0.610179 -MSE: 6.8768255508 -train accuracy: 0.685232\n",
      "epoch: 808 - cost: 0.610185 -MSE: 6.48925919362 -train accuracy: 0.666909\n",
      "epoch: 809 - cost: 0.609833 -MSE: 6.86998430231 -train accuracy: 0.686139\n",
      "epoch: 810 - cost: 0.609838 -MSE: 6.48393618983 -train accuracy: 0.667271\n",
      "epoch: 811 - cost: 0.609494 -MSE: 6.86299119477 -train accuracy: 0.686321\n",
      "epoch: 812 - cost: 0.6095 -MSE: 6.47898912047 -train accuracy: 0.667271\n",
      "epoch: 813 - cost: 0.609161 -MSE: 6.85666841893 -train accuracy: 0.686684\n",
      "epoch: 814 - cost: 0.609167 -MSE: 6.47392516182 -train accuracy: 0.667453\n",
      "epoch: 815 - cost: 0.608832 -MSE: 6.84995643887 -train accuracy: 0.687046\n",
      "epoch: 816 - cost: 0.608839 -MSE: 6.46878478438 -train accuracy: 0.667453\n",
      "epoch: 817 - cost: 0.608506 -MSE: 6.84367648256 -train accuracy: 0.686684\n",
      "epoch: 818 - cost: 0.608511 -MSE: 6.46383373677 -train accuracy: 0.667997\n",
      "epoch: 819 - cost: 0.608176 -MSE: 6.83753599036 -train accuracy: 0.686502\n",
      "epoch: 820 - cost: 0.608175 -MSE: 6.45882485327 -train accuracy: 0.668178\n",
      "epoch: 821 - cost: 0.607849 -MSE: 6.83094653246 -train accuracy: 0.686865\n",
      "epoch: 822 - cost: 0.607853 -MSE: 6.45405839164 -train accuracy: 0.66836\n",
      "epoch: 823 - cost: 0.607538 -MSE: 6.82437889479 -train accuracy: 0.687228\n",
      "epoch: 824 - cost: 0.607552 -MSE: 6.44950592189 -train accuracy: 0.668541\n",
      "epoch: 825 - cost: 0.607238 -MSE: 6.81838554568 -train accuracy: 0.687591\n",
      "epoch: 826 - cost: 0.607246 -MSE: 6.44451727135 -train accuracy: 0.668904\n",
      "epoch: 827 - cost: 0.60694 -MSE: 6.81161997816 -train accuracy: 0.687591\n",
      "epoch: 828 - cost: 0.606947 -MSE: 6.43900002085 -train accuracy: 0.669086\n",
      "epoch: 829 - cost: 0.606645 -MSE: 6.80455397293 -train accuracy: 0.687772\n",
      "epoch: 830 - cost: 0.606651 -MSE: 6.43351899606 -train accuracy: 0.669267\n",
      "epoch: 831 - cost: 0.606351 -MSE: 6.7979054166 -train accuracy: 0.687954\n",
      "epoch: 832 - cost: 0.606356 -MSE: 6.42824488692 -train accuracy: 0.669448\n",
      "epoch: 833 - cost: 0.606066 -MSE: 6.79088650516 -train accuracy: 0.687954\n",
      "epoch: 834 - cost: 0.606068 -MSE: 6.42245772811 -train accuracy: 0.669811\n",
      "epoch: 835 - cost: 0.605778 -MSE: 6.78392743551 -train accuracy: 0.688135\n",
      "epoch: 836 - cost: 0.605774 -MSE: 6.41647664828 -train accuracy: 0.670174\n",
      "epoch: 837 - cost: 0.605484 -MSE: 6.77681159253 -train accuracy: 0.687772\n",
      "epoch: 838 - cost: 0.605483 -MSE: 6.41082338517 -train accuracy: 0.670356\n",
      "epoch: 839 - cost: 0.605193 -MSE: 6.76996118997 -train accuracy: 0.687409\n",
      "epoch: 840 - cost: 0.605191 -MSE: 6.4051313247 -train accuracy: 0.670174\n",
      "epoch: 841 - cost: 0.604903 -MSE: 6.76307317212 -train accuracy: 0.688498\n",
      "epoch: 842 - cost: 0.604896 -MSE: 6.39934469296 -train accuracy: 0.670174\n",
      "epoch: 843 - cost: 0.604613 -MSE: 6.75592893516 -train accuracy: 0.689405\n",
      "epoch: 844 - cost: 0.604609 -MSE: 6.39366031772 -train accuracy: 0.670174\n",
      "epoch: 845 - cost: 0.604327 -MSE: 6.7490114652 -train accuracy: 0.689586\n",
      "epoch: 846 - cost: 0.604324 -MSE: 6.38812059163 -train accuracy: 0.670718\n",
      "epoch: 847 - cost: 0.604044 -MSE: 6.74225223134 -train accuracy: 0.689405\n",
      "epoch: 848 - cost: 0.604035 -MSE: 6.38239284028 -train accuracy: 0.671081\n",
      "epoch: 849 - cost: 0.603758 -MSE: 6.73537437144 -train accuracy: 0.689405\n",
      "epoch: 850 - cost: 0.603744 -MSE: 6.37679817918 -train accuracy: 0.671081\n",
      "epoch: 851 - cost: 0.603475 -MSE: 6.7282591136 -train accuracy: 0.689768\n",
      "epoch: 852 - cost: 0.603457 -MSE: 6.37072363577 -train accuracy: 0.671263\n",
      "epoch: 853 - cost: 0.60319 -MSE: 6.72107052944 -train accuracy: 0.690312\n",
      "epoch: 854 - cost: 0.603171 -MSE: 6.36482346354 -train accuracy: 0.671263\n",
      "epoch: 855 - cost: 0.602912 -MSE: 6.71361109022 -train accuracy: 0.690312\n",
      "epoch: 856 - cost: 0.60289 -MSE: 6.35853057288 -train accuracy: 0.671263\n",
      "epoch: 857 - cost: 0.602634 -MSE: 6.70615907588 -train accuracy: 0.690493\n",
      "epoch: 858 - cost: 0.602615 -MSE: 6.35259460362 -train accuracy: 0.671444\n",
      "epoch: 859 - cost: 0.602359 -MSE: 6.69905247637 -train accuracy: 0.690675\n",
      "epoch: 860 - cost: 0.602339 -MSE: 6.34674536396 -train accuracy: 0.671807\n",
      "epoch: 861 - cost: 0.602085 -MSE: 6.69220786303 -train accuracy: 0.690312\n",
      "epoch: 862 - cost: 0.602064 -MSE: 6.34105396879 -train accuracy: 0.671988\n",
      "epoch: 863 - cost: 0.601811 -MSE: 6.6853811577 -train accuracy: 0.690493\n",
      "epoch: 864 - cost: 0.60179 -MSE: 6.33533375602 -train accuracy: 0.672714\n",
      "epoch: 865 - cost: 0.601543 -MSE: 6.67837328175 -train accuracy: 0.690675\n",
      "epoch: 866 - cost: 0.601519 -MSE: 6.3293678937 -train accuracy: 0.67344\n",
      "epoch: 867 - cost: 0.601274 -MSE: 6.67117854933 -train accuracy: 0.690131\n",
      "epoch: 868 - cost: 0.60125 -MSE: 6.32343485045 -train accuracy: 0.673803\n",
      "epoch: 869 - cost: 0.601007 -MSE: 6.66406594708 -train accuracy: 0.690131\n",
      "epoch: 870 - cost: 0.60098 -MSE: 6.31736718445 -train accuracy: 0.673258\n",
      "epoch: 871 - cost: 0.600739 -MSE: 6.6567909722 -train accuracy: 0.690493\n",
      "epoch: 872 - cost: 0.600712 -MSE: 6.3114327692 -train accuracy: 0.673621\n",
      "epoch: 873 - cost: 0.60048 -MSE: 6.64924115342 -train accuracy: 0.690675\n",
      "epoch: 874 - cost: 0.600451 -MSE: 6.30496780238 -train accuracy: 0.673803\n",
      "epoch: 875 - cost: 0.600221 -MSE: 6.64153318365 -train accuracy: 0.690675\n",
      "epoch: 876 - cost: 0.600189 -MSE: 6.29820155118 -train accuracy: 0.674165\n",
      "epoch: 877 - cost: 0.599962 -MSE: 6.63354085894 -train accuracy: 0.691401\n",
      "epoch: 878 - cost: 0.599928 -MSE: 6.29135660775 -train accuracy: 0.674528\n",
      "epoch: 879 - cost: 0.599708 -MSE: 6.62533233324 -train accuracy: 0.691219\n",
      "epoch: 880 - cost: 0.599673 -MSE: 6.28427670641 -train accuracy: 0.675073\n",
      "epoch: 881 - cost: 0.599455 -MSE: 6.61712322024 -train accuracy: 0.691582\n",
      "epoch: 882 - cost: 0.599422 -MSE: 6.27727439373 -train accuracy: 0.675617\n",
      "epoch: 883 - cost: 0.599208 -MSE: 6.60914067591 -train accuracy: 0.692126\n",
      "epoch: 884 - cost: 0.599176 -MSE: 6.27051459106 -train accuracy: 0.675798\n",
      "epoch: 885 - cost: 0.598963 -MSE: 6.60132690699 -train accuracy: 0.692308\n",
      "epoch: 886 - cost: 0.598927 -MSE: 6.26372178806 -train accuracy: 0.675617\n",
      "epoch: 887 - cost: 0.598718 -MSE: 6.59331324803 -train accuracy: 0.692308\n",
      "epoch: 888 - cost: 0.598683 -MSE: 6.25704994038 -train accuracy: 0.67598\n",
      "epoch: 889 - cost: 0.598474 -MSE: 6.58556190498 -train accuracy: 0.692671\n",
      "epoch: 890 - cost: 0.598439 -MSE: 6.25046413989 -train accuracy: 0.676524\n",
      "epoch: 891 - cost: 0.598232 -MSE: 6.57803865797 -train accuracy: 0.692852\n",
      "epoch: 892 - cost: 0.598197 -MSE: 6.24413899184 -train accuracy: 0.676887\n",
      "epoch: 893 - cost: 0.59799 -MSE: 6.5709115255 -train accuracy: 0.693396\n",
      "epoch: 894 - cost: 0.597955 -MSE: 6.23810423806 -train accuracy: 0.676887\n",
      "epoch: 895 - cost: 0.59775 -MSE: 6.56370376067 -train accuracy: 0.693396\n",
      "epoch: 896 - cost: 0.597715 -MSE: 6.23188200288 -train accuracy: 0.677068\n",
      "epoch: 897 - cost: 0.597512 -MSE: 6.55643408748 -train accuracy: 0.693759\n",
      "epoch: 898 - cost: 0.597478 -MSE: 6.22577586849 -train accuracy: 0.677068\n",
      "epoch: 899 - cost: 0.597278 -MSE: 6.54941608262 -train accuracy: 0.693941\n",
      "epoch: 900 - cost: 0.597245 -MSE: 6.21974418458 -train accuracy: 0.677612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 901 - cost: 0.597043 -MSE: 6.54256962277 -train accuracy: 0.693941\n",
      "epoch: 902 - cost: 0.59701 -MSE: 6.21397962372 -train accuracy: 0.677794\n",
      "epoch: 903 - cost: 0.596808 -MSE: 6.53596964749 -train accuracy: 0.693941\n",
      "epoch: 904 - cost: 0.596775 -MSE: 6.20841760516 -train accuracy: 0.678338\n",
      "epoch: 905 - cost: 0.596577 -MSE: 6.52917394632 -train accuracy: 0.694122\n",
      "epoch: 906 - cost: 0.596544 -MSE: 6.20259010211 -train accuracy: 0.678701\n",
      "epoch: 907 - cost: 0.596347 -MSE: 6.52253383206 -train accuracy: 0.694122\n",
      "epoch: 908 - cost: 0.596314 -MSE: 6.19692305559 -train accuracy: 0.679064\n",
      "epoch: 909 - cost: 0.596118 -MSE: 6.51600338096 -train accuracy: 0.694122\n",
      "epoch: 910 - cost: 0.59609 -MSE: 6.19176611661 -train accuracy: 0.678882\n",
      "epoch: 911 - cost: 0.595899 -MSE: 6.50979253392 -train accuracy: 0.694666\n",
      "epoch: 912 - cost: 0.595868 -MSE: 6.18617491097 -train accuracy: 0.67852\n",
      "epoch: 913 - cost: 0.59568 -MSE: 6.50323108831 -train accuracy: 0.695029\n",
      "epoch: 914 - cost: 0.59565 -MSE: 6.18049738541 -train accuracy: 0.678701\n",
      "epoch: 915 - cost: 0.595463 -MSE: 6.49648590638 -train accuracy: 0.695573\n",
      "epoch: 916 - cost: 0.595431 -MSE: 6.17445771415 -train accuracy: 0.678882\n",
      "epoch: 917 - cost: 0.59525 -MSE: 6.48921837981 -train accuracy: 0.69521\n",
      "epoch: 918 - cost: 0.595216 -MSE: 6.16790287433 -train accuracy: 0.679245\n",
      "epoch: 919 - cost: 0.595038 -MSE: 6.48160273448 -train accuracy: 0.694848\n",
      "epoch: 920 - cost: 0.595007 -MSE: 6.16145536244 -train accuracy: 0.679245\n",
      "epoch: 921 - cost: 0.594827 -MSE: 6.47445435297 -train accuracy: 0.69521\n",
      "epoch: 922 - cost: 0.594794 -MSE: 6.15510450658 -train accuracy: 0.679064\n",
      "epoch: 923 - cost: 0.594611 -MSE: 6.46776607506 -train accuracy: 0.69521\n",
      "epoch: 924 - cost: 0.594582 -MSE: 6.14950223989 -train accuracy: 0.679608\n",
      "epoch: 925 - cost: 0.5944 -MSE: 6.46137735832 -train accuracy: 0.69521\n",
      "epoch: 926 - cost: 0.59437 -MSE: 6.14396578237 -train accuracy: 0.680152\n",
      "epoch: 927 - cost: 0.594189 -MSE: 6.4550206721 -train accuracy: 0.695392\n",
      "epoch: 928 - cost: 0.594157 -MSE: 6.13835380609 -train accuracy: 0.679971\n",
      "epoch: 929 - cost: 0.593977 -MSE: 6.448569041 -train accuracy: 0.69521\n",
      "epoch: 930 - cost: 0.593947 -MSE: 6.13284082357 -train accuracy: 0.680334\n",
      "epoch: 931 - cost: 0.593768 -MSE: 6.44219559791 -train accuracy: 0.69521\n",
      "epoch: 932 - cost: 0.593741 -MSE: 6.12757784924 -train accuracy: 0.67979\n",
      "epoch: 933 - cost: 0.593574 -MSE: 6.43545700471 -train accuracy: 0.695392\n",
      "epoch: 934 - cost: 0.593544 -MSE: 6.12157730015 -train accuracy: 0.680334\n",
      "epoch: 935 - cost: 0.593377 -MSE: 6.42878349942 -train accuracy: 0.695573\n",
      "epoch: 936 - cost: 0.593346 -MSE: 6.11566662717 -train accuracy: 0.680334\n",
      "epoch: 937 - cost: 0.593181 -MSE: 6.42208291974 -train accuracy: 0.695392\n",
      "epoch: 938 - cost: 0.593149 -MSE: 6.10980446519 -train accuracy: 0.680152\n",
      "epoch: 939 - cost: 0.592986 -MSE: 6.41538107855 -train accuracy: 0.695392\n",
      "epoch: 940 - cost: 0.592953 -MSE: 6.1039784621 -train accuracy: 0.680152\n",
      "epoch: 941 - cost: 0.592795 -MSE: 6.40862418833 -train accuracy: 0.695029\n",
      "epoch: 942 - cost: 0.592765 -MSE: 6.09795875656 -train accuracy: 0.680334\n",
      "epoch: 943 - cost: 0.592607 -MSE: 6.40180886845 -train accuracy: 0.695392\n",
      "epoch: 944 - cost: 0.592576 -MSE: 6.09188560724 -train accuracy: 0.680697\n",
      "epoch: 945 - cost: 0.592416 -MSE: 6.39525725876 -train accuracy: 0.695573\n",
      "epoch: 946 - cost: 0.592384 -MSE: 6.08611601682 -train accuracy: 0.681241\n",
      "epoch: 947 - cost: 0.592224 -MSE: 6.38876601498 -train accuracy: 0.69521\n",
      "epoch: 948 - cost: 0.592191 -MSE: 6.08040433174 -train accuracy: 0.681241\n",
      "epoch: 949 - cost: 0.592035 -MSE: 6.3823620492 -train accuracy: 0.695573\n",
      "epoch: 950 - cost: 0.592 -MSE: 6.07479748413 -train accuracy: 0.681604\n",
      "epoch: 951 - cost: 0.591847 -MSE: 6.3759422598 -train accuracy: 0.695392\n",
      "epoch: 952 - cost: 0.591811 -MSE: 6.06916387095 -train accuracy: 0.681785\n",
      "epoch: 953 - cost: 0.59166 -MSE: 6.36970711797 -train accuracy: 0.695392\n",
      "epoch: 954 - cost: 0.591626 -MSE: 6.06391827782 -train accuracy: 0.681604\n",
      "epoch: 955 - cost: 0.591475 -MSE: 6.36373985263 -train accuracy: 0.695755\n",
      "epoch: 956 - cost: 0.59144 -MSE: 6.05865363046 -train accuracy: 0.681967\n",
      "epoch: 957 - cost: 0.591289 -MSE: 6.35777535347 -train accuracy: 0.696118\n",
      "epoch: 958 - cost: 0.591253 -MSE: 6.05349567073 -train accuracy: 0.681785\n",
      "epoch: 959 - cost: 0.591105 -MSE: 6.35207134911 -train accuracy: 0.69648\n",
      "epoch: 960 - cost: 0.591068 -MSE: 6.04855459775 -train accuracy: 0.682148\n",
      "epoch: 961 - cost: 0.590921 -MSE: 6.34646176738 -train accuracy: 0.696843\n",
      "epoch: 962 - cost: 0.590886 -MSE: 6.04394151996 -train accuracy: 0.682874\n",
      "epoch: 963 - cost: 0.590744 -MSE: 6.34098489722 -train accuracy: 0.697025\n",
      "epoch: 964 - cost: 0.59071 -MSE: 6.0393652555 -train accuracy: 0.683418\n",
      "epoch: 965 - cost: 0.59057 -MSE: 6.33544028111 -train accuracy: 0.697206\n",
      "epoch: 966 - cost: 0.590536 -MSE: 6.0346676554 -train accuracy: 0.683418\n",
      "epoch: 967 - cost: 0.590398 -MSE: 6.32999135914 -train accuracy: 0.697388\n",
      "epoch: 968 - cost: 0.590363 -MSE: 6.0300787915 -train accuracy: 0.683599\n",
      "epoch: 969 - cost: 0.590226 -MSE: 6.32483460755 -train accuracy: 0.697388\n",
      "epoch: 970 - cost: 0.590189 -MSE: 6.02552744813 -train accuracy: 0.684144\n",
      "epoch: 971 - cost: 0.590055 -MSE: 6.31938956601 -train accuracy: 0.697569\n",
      "epoch: 972 - cost: 0.590018 -MSE: 6.02087362529 -train accuracy: 0.684144\n",
      "epoch: 973 - cost: 0.589887 -MSE: 6.31402975693 -train accuracy: 0.69775\n",
      "epoch: 974 - cost: 0.589852 -MSE: 6.01645577306 -train accuracy: 0.684144\n",
      "epoch: 975 - cost: 0.589724 -MSE: 6.30879354269 -train accuracy: 0.698113\n",
      "epoch: 976 - cost: 0.589687 -MSE: 6.01168681175 -train accuracy: 0.684144\n",
      "epoch: 977 - cost: 0.589563 -MSE: 6.3033624869 -train accuracy: 0.698295\n",
      "epoch: 978 - cost: 0.589529 -MSE: 6.00711923715 -train accuracy: 0.684325\n",
      "epoch: 979 - cost: 0.58941 -MSE: 6.29800913377 -train accuracy: 0.698476\n",
      "epoch: 980 - cost: 0.589374 -MSE: 6.00241575845 -train accuracy: 0.684507\n",
      "epoch: 981 - cost: 0.589257 -MSE: 6.29250187587 -train accuracy: 0.698657\n",
      "epoch: 982 - cost: 0.589221 -MSE: 5.99752726985 -train accuracy: 0.684688\n",
      "epoch: 983 - cost: 0.589108 -MSE: 6.28698875645 -train accuracy: 0.69902\n",
      "epoch: 984 - cost: 0.589072 -MSE: 5.99268981054 -train accuracy: 0.685232\n",
      "epoch: 985 - cost: 0.588959 -MSE: 6.28160183348 -train accuracy: 0.699383\n",
      "epoch: 986 - cost: 0.588924 -MSE: 5.98793272943 -train accuracy: 0.685051\n",
      "epoch: 987 - cost: 0.588816 -MSE: 6.27603450424 -train accuracy: 0.699927\n",
      "epoch: 988 - cost: 0.58878 -MSE: 5.98290218048 -train accuracy: 0.685232\n",
      "epoch: 989 - cost: 0.588674 -MSE: 6.27036951911 -train accuracy: 0.699746\n",
      "epoch: 990 - cost: 0.588637 -MSE: 5.97783501586 -train accuracy: 0.685051\n",
      "epoch: 991 - cost: 0.588534 -MSE: 6.26467020792 -train accuracy: 0.699927\n",
      "epoch: 992 - cost: 0.588495 -MSE: 5.97271375557 -train accuracy: 0.685595\n",
      "epoch: 993 - cost: 0.588395 -MSE: 6.25890242753 -train accuracy: 0.699927\n",
      "epoch: 994 - cost: 0.588355 -MSE: 5.96745520917 -train accuracy: 0.685776\n",
      "epoch: 995 - cost: 0.588257 -MSE: 6.25300982108 -train accuracy: 0.699565\n",
      "epoch: 996 - cost: 0.588216 -MSE: 5.96211701029 -train accuracy: 0.686139\n",
      "epoch: 997 - cost: 0.588127 -MSE: 6.24656012731 -train accuracy: 0.699565\n",
      "epoch: 998 - cost: 0.588086 -MSE: 5.95620358111 -train accuracy: 0.686139\n",
      "epoch: 999 - cost: 0.587989 -MSE: 6.24074110873 -train accuracy: 0.699927\n"
     ]
    }
   ],
   "source": [
    "training_epochs=1000\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    sess.run(train_step,feed_dict={x:train_data,y_:train_labels})\n",
    "    cost=sess.run(cross_entropy,feed_dict={x:train_data,y_:train_labels})\n",
    "    cost_history=np.append(cost_history,cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    pred_y=sess.run(y,feed_dict={x:test_data})\n",
    "    mse=tf.reduce_mean(tf.square(pred_y - test_labels))\n",
    "    mse_=sess.run(mse)\n",
    "    mse_history.append(mse_)\n",
    "    accuracy=sess.run(accuracy,feed_dict={x:train_data,y_:train_labels})\n",
    "    accuracy_history.append(accuracy)\n",
    "    \n",
    "    print('epoch:',epoch,'-','cost:',cost,'-MSE:',mse_,'-train accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.686502\n"
     ]
    }
   ],
   "source": [
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print('test accuracy:',sess.run(accuracy,feed_dict={x:test_data,y_:test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:6.2407\n"
     ]
    }
   ],
   "source": [
    "pred_y=sess.run(y,feed_dict={x:test_data})\n",
    "mse=tf.reduce_mean(tf.square(pred_y-test_labels))\n",
    "print('mse:%.4f'%sess.run(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score,accuracy_score\n",
    "#print(f1_score(test_labels,predictions),accuracy_score(test_labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
